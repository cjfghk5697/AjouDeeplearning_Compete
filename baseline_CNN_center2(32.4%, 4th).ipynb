{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "baseline_CNN_center2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cjfghk5697/AjouDeeplearning_Compete/blob/main/baseline_CNN_center2(32.4%25%2C%204th).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1IyBAj-U9zh",
        "outputId": "00f7aad5-33b4-4a67-f58c-b82a17944bce"
      },
      "source": [
        "!pip install git+https://github.com/cmpark0126/pytorch-polynomial-lr-decay.git"
      ],
      "id": "M1IyBAj-U9zh",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/cmpark0126/pytorch-polynomial-lr-decay.git\n",
            "  Cloning https://github.com/cmpark0126/pytorch-polynomial-lr-decay.git to /tmp/pip-req-build-yqf5jut6\n",
            "  Running command git clone -q https://github.com/cmpark0126/pytorch-polynomial-lr-decay.git /tmp/pip-req-build-yqf5jut6\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torch-poly-lr-decay==0.0.1) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torch-poly-lr-decay==0.0.1) (3.7.4.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXB81yLssTRi",
        "outputId": "7dafa295-bc2f-4bbe-e12b-a780e85cce6b"
      },
      "source": [
        "!pip3 install timm"
      ],
      "id": "iXB81yLssTRi",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (0.4.12)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.9.0+cu102)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.10.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b610b1cf",
        "scrolled": true
      },
      "source": [
        "import gdown\n",
        "global PATH\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import logging\n",
        "import random\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from google.colab import files\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "from torchsummary import summary\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "\n",
        "import pandas as pd\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "import albumentations as A\n",
        "import cv2\n",
        "from glob import glob\n",
        "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "import timm\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "\n",
        "url_path_map = {\n",
        "        'train': {\n",
        "            'url':'https://drive.google.com/uc?id=1JewlmOsqs-O1EmMMKutcYYiD7GkN6brN', \n",
        "            'path':'dataset/train.npy'\n",
        "            },\n",
        "        'test': {\n",
        "            'url':'https://drive.google.com/uc?id=1JcCFZbc_N7VIa3G3XA8grwz4XGdaaXSZ', \n",
        "            'path':'dataset/test.npy'\n",
        "            },\n",
        "        'label': {\n",
        "            'url':'https://drive.google.com/uc?id=1JiaTOcZ6QfDThw3RrIa3ZxDDaNt-BMUQ',\n",
        "            'path':'dataset/label_info.txt'\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "def download_dataset():\n",
        "    Path('dataset').mkdir(exist_ok=True)\n",
        "    for split in ['train', 'test', 'label']:\n",
        "        if Path(url_path_map[split]['path']).exists():\n",
        "            continue\n",
        "        gdown.download(url_path_map[split]['url'], url_path_map[split]['path'], quiet=False)\n",
        "\n",
        "\n",
        "\n",
        "def read_txt(file_name):\n",
        "    with open(file_name, 'rt') as f:\n",
        "        class_list = [row.strip('\\n') for row in f.readlines()]\n",
        "    return class_list\n",
        "\n",
        "\n",
        "class MyDataset:\n",
        "    def __init__(self, root, split='train', transform_fn=None, label='label_info'):\n",
        "        self.dataset = np.load('{}/{}.npy'.format(root, split), allow_pickle=True).item()\n",
        "        self.label_name = read_txt('{}/{}.txt'.format(root, label))\n",
        "        self.transform_fn = transform_fn\n",
        "        self.split = split\n",
        "    \n",
        "    @property\n",
        "    def class_num(self):\n",
        "        return len(self.label_name)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.dataset['label'])\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.dataset['image'][idx], self.dataset['label'][idx]\n",
        "        \n",
        "        if self.transform_fn:\n",
        "            image = self.transform_fn(image)\n",
        "        \n",
        "        return image, label"
      ],
      "id": "b610b1cf",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGJZ2JW9jWFJ"
      },
      "source": [
        "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
        "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = x.size()[0]\n",
        "    if use_cuda:\n",
        "        index = torch.randperm(batch_size).cuda()\n",
        "    else:\n",
        "        index = torch.randperm(batch_size)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
      ],
      "id": "oGJZ2JW9jWFJ",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh2g60Y-xBLx"
      },
      "source": [
        "def seed_everything(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)  # type: ignore\n",
        "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
        "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
        "seed_everything(42)"
      ],
      "id": "uh2g60Y-xBLx",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn5MRjporv6O"
      },
      "source": [
        "\n",
        "class RAdam(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        \n",
        "        self.degenerated_to_sgd = degenerated_to_sgd\n",
        "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
        "            for param in params:\n",
        "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
        "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, buffer=[[None, None, None] for _ in range(10)])\n",
        "        super(RAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(RAdam, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "                else:\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "\n",
        "                state['step'] += 1\n",
        "                buffered = group['buffer'][int(state['step'] % 10)]\n",
        "                if state['step'] == buffered[0]:\n",
        "                    N_sma, step_size = buffered[1], buffered[2]\n",
        "                else:\n",
        "                    buffered[0] = state['step']\n",
        "                    beta2_t = beta2 ** state['step']\n",
        "                    N_sma_max = 2 / (1 - beta2) - 1\n",
        "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
        "                    buffered[1] = N_sma\n",
        "\n",
        "                    # more conservative since it's an approximated value\n",
        "                    if N_sma >= 5:\n",
        "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
        "                    elif self.degenerated_to_sgd:\n",
        "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
        "                    else:\n",
        "                        step_size = -1\n",
        "                    buffered[2] = step_size\n",
        "\n",
        "                # more conservative since it's an approximated value\n",
        "                if N_sma >= 5:\n",
        "                    if group['weight_decay'] != 0:\n",
        "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n",
        "                    p.data.copy_(p_data_fp32)\n",
        "                elif step_size > 0:\n",
        "                    if group['weight_decay'] != 0:\n",
        "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
        "                    p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss"
      ],
      "id": "Fn5MRjporv6O",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ1qFxNw1-Ey"
      },
      "source": [
        "class MnistModel_efficientb7(nn.Module):\n",
        "    def __init__(self,model, in_channel=30, out_channel=3):\n",
        "        super().__init__()\n",
        "        self.model=model\n",
        "        self.in_channel = in_channel\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        self.conv0 = nn.Conv2d(in_channel, out_channel, kernel_size=(3, 3),padding=(1,1), stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv0(x)\n",
        "        x = self.model(x)\n",
        "\n",
        "        return x"
      ],
      "id": "IQ1qFxNw1-Ey",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMyhgG1tVbM5"
      },
      "source": [
        "class MyModelWrapper:\n",
        "    def __init__(self, log_name, model, criterion, optimizer, scheduler ,device):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "\n",
        "        self.device = device\n",
        "        self.best_acc = 0\n",
        "        self.best_loss=100\n",
        "        self.setup_directory()\n",
        "        self.log_name = '{}-{}'.format(datetime.datetime.now().strftime('%Y-%m-%d/%H-%M-%S'), log_name)\n",
        "        self.log_best_weight_path = 'log/best_weight/{}.pth'.format(self.log_name)\n",
        "        self.writer = SummaryWriter(log_dir='log/tensor_board/{}'.format(self.log_name), filename_suffix=log_name)\n",
        "        self.log_loss_weight_path = 'log/loss_weight/{}.pth'.format(self.log_name)\n",
        "    \n",
        "    def setup_directory(self):\n",
        "        Path('log/tensor_board/{}'.format(datetime.datetime.now().strftime('%Y-%m-%d'))).mkdir(exist_ok=True, parents=True)\n",
        "        Path('log/best_weight/{}'.format(datetime.datetime.now().strftime('%Y-%m-%d'))).mkdir(exist_ok=True, parents=True)\n",
        "        Path('log/loss_weight/{}'.format(datetime.datetime.now().strftime('%Y-%m-%d'))).mkdir(exist_ok=True, parents=True)\n",
        "    \n",
        "    def train(self, train_dl, epoch):\n",
        "        debug_step = len(train_dl)//10\n",
        "        batch_time = AverageMeter('Time', ':6.3f')\n",
        "        data_time = AverageMeter('Data', ':6.3f')\n",
        "        losses = AverageMeter('Loss', ':7.4f')\n",
        "        top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "        top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "        progress = ProgressMeter(\n",
        "            len(train_dl),\n",
        "            [batch_time, data_time, losses, top1, top5],\n",
        "            prefix=\"TRAIN: [{}]\".format(epoch))\n",
        "\n",
        "        self.model.train()\n",
        "\n",
        "        end = time.time()\n",
        "        for step, (x, y) in enumerate(train_dl):\n",
        "            data_time.update(time.time() - end)\n",
        "            x, y = x.to(self.device), y.to(self.device)\n",
        "        \n",
        "            x, y_a, y_b, lam = mixup_data(x,y)\n",
        "            x, y_a, y_b = map(Variable, (x, y_a, y_b))\n",
        "            y_hat = self.model(x)\n",
        "            loss = mixup_criterion(self.criterion, y_hat, y_a, y_b, lam)\n",
        "\n",
        "            acc1, acc5 = accuracy(y_hat, y, topk=(1, 5))\n",
        "            losses.update(loss.item(), x.size(0))\n",
        "            top1.update(acc1[0], x.size(0))\n",
        "            top5.update(acc5[0], x.size(0))\n",
        "\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if step != 0 and step % debug_step == 0:\n",
        "                progress.display(step)\n",
        "        return losses.avg, top1.avg\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def valid(self, dl):\n",
        "        debug_step = len(dl) // 10\n",
        "        batch_time = AverageMeter('Time', ':6.3f')\n",
        "        losses = AverageMeter('Loss', ':7.4f')\n",
        "        top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "        top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "        progress = ProgressMeter(\n",
        "            len(dl),\n",
        "            [batch_time, losses, top1, top5],\n",
        "            prefix='VALID: ')\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        end = time.time()\n",
        "        for step, (x, y) in enumerate(dl):\n",
        "            x, y = x.float().to(self.device), y.long().to(self.device)\n",
        "            y_hat = self.model(x)\n",
        "            loss = F.cross_entropy(y_hat, y)\n",
        "\n",
        "            acc1, acc5 = accuracy(y_hat, y, topk=(1, 5))\n",
        "            losses.update(loss.item(), x.size(0))\n",
        "            top1.update(acc1[0], x.size(0))\n",
        "            top5.update(acc5[0], x.size(0))\n",
        "\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if step % debug_step == 0:\n",
        "                progress.display(step)\n",
        "\n",
        "        return losses.avg, top1.avg\n",
        "\n",
        "    \n",
        "    def fit(self, train_dl,valid_dl, nepoch=40):\n",
        "        no_improvement=0\n",
        "        best_acc = 0\n",
        "        best_acc_arg = 0\n",
        "        best_loss=100\n",
        "        \n",
        "        for epoch in range(nepoch):\n",
        "            train_loss, train_acc = self.train(train_dl, epoch)\n",
        "            valid_loss, valid_acc = self.train(valid_dl, epoch)\n",
        "            flag=0\n",
        "            if valid_acc > best_acc:\n",
        "                best_acc = valid_acc\n",
        "                best_acc_arg = epoch + 1\n",
        "                self.save_best_weight(self.model, best_acc)\n",
        "                flag += 1\n",
        "                no_improvement = 0\n",
        "            \n",
        "            \n",
        "            if valid_loss < best_loss:\n",
        "                best_loss= valid_loss\n",
        "                self.save_best_loss_weight(self.model, best_loss)\n",
        "                flag += 1\n",
        "                no_improvement = 0\n",
        "    \n",
        "            if flag == 0:\n",
        "              no_improvement += 1\n",
        "    \n",
        "            if no_improvement > 30:\n",
        "              print(\"No improvement for 3 epochs, stopping\")\n",
        "              break\n",
        "            self.scheduler.step()\n",
        "            \n",
        "            print('=' * 150)\n",
        "            print(\n",
        "                '[EPOCH]: {:03d}/{:03d}    train loss {:07.4f} acc {:07.4f}% ('\n",
        "                'best accuracy : {:07.4f} @ {:03d})'.format(\n",
        "                    epoch + 1, nepoch, valid_loss, valid_acc, best_acc,\n",
        "                    best_acc_arg\n",
        "                ))\n",
        "            print('=' * 150)\n",
        "            self.log_tensorboard(epoch, valid_loss, valid_acc)\n",
        "            \n",
        "      \n",
        "\n",
        "\n",
        "    def save_best_weight(self, model, top1_acc):\n",
        "        global PATH\n",
        "        if top1_acc > self.best_acc:\n",
        "            self.best_acc = top1_acc\n",
        "            print('Saving best model({:07.4f}%) weight to {}'.format(top1_acc, self.log_best_weight_path))\n",
        "            torch.save({'weight':model.state_dict(), 'top1_acc':top1_acc}, self.log_best_weight_path)\n",
        "            PATH=self.log_best_weight_path\n",
        "    \n",
        "    def save_best_loss_weight(self, model, top1_loss):\n",
        "        global loss_PATH\n",
        "        if top1_loss < self.best_loss:\n",
        "            self.best_loss = top1_loss\n",
        "            print('Saving best loss model({:07.4f}%) weight to {}'.format(top1_loss, self.log_loss_weight_path))\n",
        "            torch.save({'weight':model.state_dict(), 'top1_acc':top1_loss}, self.log_loss_weight_path)\n",
        "\n",
        "            loss_PATH=self.log_loss_weight_path\n",
        "\n",
        "\n",
        "    def log_tensorboard(self, epoch, train_loss, train_acc):\n",
        "        self.writer.add_scalar('Loss/train', train_loss, epoch)\n",
        "        self.writer.add_scalar('Accuracy/train', train_acc, epoch)"
      ],
      "id": "hMyhgG1tVbM5",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6VvxCTstkMC"
      },
      "source": [
        "class MyModelWrapper_none:\n",
        "    def __init__(self, log_name, model, criterion, optimizer ,device):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        self.device = device\n",
        "        self.best_acc = 0\n",
        "        self.best_loss=100\n",
        "        self.setup_directory()\n",
        "        self.log_name = '{}-{}'.format(datetime.datetime.now().strftime('%Y-%m-%d/%H-%M-%S'), log_name)\n",
        "        self.log_best_weight_path = 'log/best_weight/{}.pth'.format(self.log_name)\n",
        "        self.writer = SummaryWriter(log_dir='log/tensor_board/{}'.format(self.log_name), filename_suffix=log_name)\n",
        "        self.log_loss_weight_path = 'log/loss_weight/{}.pth'.format(self.log_name)\n",
        "    \n",
        "    def setup_directory(self):\n",
        "        Path('log/tensor_board/{}'.format(datetime.datetime.now().strftime('%Y-%m-%d'))).mkdir(exist_ok=True, parents=True)\n",
        "        Path('log/best_weight/{}'.format(datetime.datetime.now().strftime('%Y-%m-%d'))).mkdir(exist_ok=True, parents=True)\n",
        "        Path('log/loss_weight/{}'.format(datetime.datetime.now().strftime('%Y-%m-%d'))).mkdir(exist_ok=True, parents=True)\n",
        "    \n",
        "    def train(self, train_dl, epoch):\n",
        "        debug_step = len(train_dl)//10\n",
        "        batch_time = AverageMeter('Time', ':6.3f')\n",
        "        data_time = AverageMeter('Data', ':6.3f')\n",
        "        losses = AverageMeter('Loss', ':7.4f')\n",
        "        top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "        top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "        progress = ProgressMeter(\n",
        "            len(train_dl),\n",
        "            [batch_time, data_time, losses, top1, top5],\n",
        "            prefix=\"TRAIN: [{}]\".format(epoch))\n",
        "\n",
        "        self.model.train()\n",
        "\n",
        "        end = time.time()\n",
        "        for step, (x, y) in enumerate(train_dl):\n",
        "            data_time.update(time.time() - end)\n",
        "            x, y = x.to(self.device), y.to(self.device)\n",
        "        \n",
        "            x, y_a, y_b, lam = mixup_data(x,y)\n",
        "            x, y_a, y_b = map(Variable, (x, y_a, y_b))\n",
        "            y_hat = self.model(x)\n",
        "            loss = mixup_criterion(self.criterion, y_hat, y_a, y_b, lam)\n",
        "\n",
        "            acc1, acc5 = accuracy(y_hat, y, topk=(1, 5))\n",
        "            losses.update(loss.item(), x.size(0))\n",
        "            top1.update(acc1[0], x.size(0))\n",
        "            top5.update(acc5[0], x.size(0))\n",
        "\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if step != 0 and step % debug_step == 0:\n",
        "                progress.display(step)\n",
        "        return losses.avg, top1.avg\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def valid(self, dl):\n",
        "        debug_step = len(dl) // 10\n",
        "        batch_time = AverageMeter('Time', ':6.3f')\n",
        "        losses = AverageMeter('Loss', ':7.4f')\n",
        "        top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "        top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "        progress = ProgressMeter(\n",
        "            len(dl),\n",
        "            [batch_time, losses, top1, top5],\n",
        "            prefix='VALID: ')\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        end = time.time()\n",
        "        for step, (x, y) in enumerate(dl):\n",
        "            x, y = x.float().to(self.device), y.long().to(self.device)\n",
        "            y_hat = self.model(x)\n",
        "            loss = F.cross_entropy(y_hat, y)\n",
        "\n",
        "            acc1, acc5 = accuracy(y_hat, y, topk=(1, 5))\n",
        "            losses.update(loss.item(), x.size(0))\n",
        "            top1.update(acc1[0], x.size(0))\n",
        "            top5.update(acc5[0], x.size(0))\n",
        "\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if step % debug_step == 0:\n",
        "                progress.display(step)\n",
        "\n",
        "        return losses.avg, top1.avg\n",
        "\n",
        "    \n",
        "    def fit(self, train_dl,valid_dl, nepoch):\n",
        "        no_improvement=0\n",
        "        best_acc = 0\n",
        "        best_acc_arg = 0\n",
        "        best_loss=100\n",
        "        \n",
        "        for epoch in range(nepoch):\n",
        "            train_loss, train_acc = self.train(train_dl, epoch)\n",
        "            valid_loss, valid_acc = self.train(valid_dl, epoch)\n",
        "            flag=0\n",
        "            if valid_acc > best_acc:\n",
        "                best_acc = valid_acc\n",
        "                best_acc_arg = epoch + 1\n",
        "                self.save_best_weight(self.model, best_acc)\n",
        "                flag += 1\n",
        "                no_improvement = 0\n",
        "\n",
        "            \n",
        "            if valid_loss < best_loss:\n",
        "                best_loss= valid_loss\n",
        "                self.save_best_loss_weight(self.model, best_loss)\n",
        "                flag += 1\n",
        "                no_improvement = 0\n",
        "    \n",
        "            if flag == 0:\n",
        "              no_improvement += 1\n",
        "    \n",
        "            if no_improvement > 30:\n",
        "              print(\"No improvement for 3 epochs, stopping\")\n",
        "              break\n",
        "            \n",
        "            print('=' * 150)\n",
        "            print(\n",
        "                '[EPOCH]: {:03d}/{:03d}    train loss {:07.4f} acc {:07.4f}% ('\n",
        "                'best accuracy : {:07.4f} @ {:03d})'.format(\n",
        "                    epoch + 1, nepoch, valid_loss, valid_acc, best_acc,\n",
        "                    best_acc_arg\n",
        "                ))\n",
        "            print('=' * 150)\n",
        "            self.log_tensorboard(epoch, valid_loss, valid_acc)\n",
        "            \n",
        "      \n",
        "\n",
        "\n",
        "    def save_best_weight(self, model, top1_acc):\n",
        "        global PATH\n",
        "        if top1_acc > self.best_acc:\n",
        "            self.best_acc = top1_acc\n",
        "            print('Saving best model({:07.4f}%) weight to {}'.format(top1_acc, self.log_best_weight_path))\n",
        "            torch.save({'weight':model.state_dict(), 'top1_acc':top1_acc}, self.log_best_weight_path)\n",
        "            PATH=self.log_best_weight_path\n",
        "    \n",
        "    def save_best_loss_weight(self, model, top1_loss):\n",
        "        global loss_PATH\n",
        "        if top1_loss < self.best_loss:\n",
        "            self.best_loss = top1_loss\n",
        "            print('Saving best loss model({:07.4f}%) weight to {}'.format(top1_loss, self.log_loss_weight_path))\n",
        "            torch.save({'weight':model.state_dict(), 'top1_acc':top1_loss}, self.log_loss_weight_path)\n",
        "            loss_PATH=self.log_loss_weight_path\n",
        "\n",
        "\n",
        "    def log_tensorboard(self, epoch, train_loss, train_acc):\n",
        "        self.writer.add_scalar('Loss/train', train_loss, epoch)\n",
        "        self.writer.add_scalar('Accuracy/train', train_acc, epoch)"
      ],
      "id": "C6VvxCTstkMC",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_shraj2Ei7zp"
      },
      "source": [
        "class CosineAnnealingWarmupRestarts(_LRScheduler):\n",
        "    \"\"\"\n",
        "        optimizer (Optimizer): Wrapped optimizer.\n",
        "        first_cycle_steps (int): First cycle step size.\n",
        "        cycle_mult(float): Cycle steps magnification. Default: -1.\n",
        "        max_lr(float): First cycle's max learning rate. Default: 0.1.\n",
        "        min_lr(float): Min learning rate. Default: 0.001.\n",
        "        warmup_steps(int): Linear warmup step size. Default: 0.\n",
        "        gamma(float): Decrease rate of max learning rate by cycle. Default: 1.\n",
        "        last_epoch (int): The index of last epoch. Default: -1.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self,\n",
        "                 optimizer : torch.optim.Optimizer,\n",
        "                 first_cycle_steps : int,\n",
        "                 cycle_mult : float = 1.,\n",
        "                 max_lr : float = 0.1,\n",
        "                 min_lr : float = 0.001,\n",
        "                 warmup_steps : int = 0,\n",
        "                 gamma : float = 1.,\n",
        "                 last_epoch : int = -1\n",
        "        ):\n",
        "        assert warmup_steps < first_cycle_steps\n",
        "        \n",
        "        self.first_cycle_steps = first_cycle_steps # first cycle step size\n",
        "        self.cycle_mult = cycle_mult # cycle steps magnification\n",
        "        self.base_max_lr = max_lr # first max learning rate\n",
        "        self.max_lr = max_lr # max learning rate in the current cycle\n",
        "        self.min_lr = min_lr # min learning rate\n",
        "        self.warmup_steps = warmup_steps # warmup step size\n",
        "        self.gamma = gamma # decrease rate of max learning rate by cycle\n",
        "        \n",
        "        self.cur_cycle_steps = first_cycle_steps # first cycle step size\n",
        "        self.cycle = 0 # cycle count\n",
        "        self.step_in_cycle = last_epoch # step size of the current cycle\n",
        "        \n",
        "        super(CosineAnnealingWarmupRestarts, self).__init__(optimizer, last_epoch)\n",
        "        \n",
        "        # set learning rate min_lr\n",
        "        self.init_lr()\n",
        "    \n",
        "    def init_lr(self):\n",
        "        self.base_lrs = []\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = self.min_lr\n",
        "            self.base_lrs.append(self.min_lr)\n",
        "    \n",
        "    def get_lr(self):\n",
        "        if self.step_in_cycle == -1:\n",
        "            return self.base_lrs\n",
        "        elif self.step_in_cycle < self.warmup_steps:\n",
        "            return [(self.max_lr - base_lr)*self.step_in_cycle / self.warmup_steps + base_lr for base_lr in self.base_lrs]\n",
        "        else:\n",
        "            return [base_lr + (self.max_lr - base_lr) \\\n",
        "                    * (1 + math.cos(math.pi * (self.step_in_cycle-self.warmup_steps) \\\n",
        "                                    / (self.cur_cycle_steps - self.warmup_steps))) / 2\n",
        "                    for base_lr in self.base_lrs]\n",
        "\n",
        "    def step(self, epoch=None):\n",
        "        if epoch is None:\n",
        "            epoch = self.last_epoch + 1\n",
        "            self.step_in_cycle = self.step_in_cycle + 1\n",
        "            if self.step_in_cycle >= self.cur_cycle_steps:\n",
        "                self.cycle += 1\n",
        "                self.step_in_cycle = self.step_in_cycle - self.cur_cycle_steps\n",
        "                self.cur_cycle_steps = int((self.cur_cycle_steps - self.warmup_steps) * self.cycle_mult) + self.warmup_steps\n",
        "        else:\n",
        "            if epoch >= self.first_cycle_steps:\n",
        "                if self.cycle_mult == 1.:\n",
        "                    self.step_in_cycle = epoch % self.first_cycle_steps\n",
        "                    self.cycle = epoch // self.first_cycle_steps\n",
        "                else:\n",
        "                    n = int(math.log((epoch / self.first_cycle_steps * (self.cycle_mult - 1) + 1), self.cycle_mult))\n",
        "                    self.cycle = n\n",
        "                    self.step_in_cycle = epoch - int(self.first_cycle_steps * (self.cycle_mult ** n - 1) / (self.cycle_mult - 1))\n",
        "                    self.cur_cycle_steps = self.first_cycle_steps * self.cycle_mult ** (n)\n",
        "            else:\n",
        "                self.cur_cycle_steps = self.first_cycle_steps\n",
        "                self.step_in_cycle = epoch\n",
        "                \n",
        "        self.max_lr = self.base_max_lr * (self.gamma**self.cycle)\n",
        "        self.last_epoch = math.floor(epoch)\n",
        "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
        "            param_group['lr'] = lr"
      ],
      "id": "_shraj2Ei7zp",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YF7wHIKVjQf"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n"
      ],
      "id": "1YF7wHIKVjQf",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4SmZwkSQ1na"
      },
      "source": [
        "!CUDA_LAUNCH_BLOCKING=1"
      ],
      "id": "o4SmZwkSQ1na",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_sqAVzUQ3vp"
      },
      "source": [
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "id": "e_sqAVzUQ3vp",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKMvBchwI5Lc"
      },
      "source": [
        "def run_vit_large(model_name='vit_large', lr=1e-5, batch_size=8, nepoch=8):\n",
        "\n",
        "    # step 1. load dataset\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.RandomCrop(224),\n",
        "        transforms.RandomHorizontalFlip(p=0.2),\n",
        "        transforms.RandomVerticalFlip(p=0.2),\n",
        "        transforms.RandomRotation(30),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406)*10, (0.229, 0.224, 0.225)*10),\n",
        "    ])\n",
        "\n",
        "    train = MyDataset('dataset', 'train', train_transforms)\n",
        "\n",
        "    train_size = int(0.7 * len(train))\n",
        "    valid_size = len(train)-train_size\n",
        "\n",
        "    train_dataset, val_dataset = random_split(train,[train_size,valid_size])\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, shuffle=True, batch_size=16, num_workers=8)\n",
        "    valid_loader = torch.utils.data.DataLoader(dataset=val_dataset,shuffle=False, batch_size=8,num_workers=4)\n",
        "\n",
        "    # step 2. load model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = get_model(model_name).to(device)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # step 3. prepare training tool\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
        "    decay_steps = (len(train_dataset)//batch_size)*nepoch\n",
        "    scheduler = PolynomialLRDecay(optimizer, max_decay_steps=decay_steps, end_learning_rate=1e-6, power=0.9)\n",
        "    # step 4. train model\n",
        "    model = MyModelWrapper(model_name, model=model, device=device, optimizer=optimizer, criterion=criterion,scheduler=scheduler)\n",
        "    model.fit(train_loader,valid_loader, nepoch=nepoch)\n",
        "def run_vit_base(model_name='vit_base', lr=1e-4, batch_size=8, nepoch=100):\n",
        "    download_dataset()\n",
        "    # step 1. load dataset\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.RandomCrop(224),\n",
        "        transforms.RandomHorizontalFlip(p=0.2),\n",
        "        transforms.RandomVerticalFlip(p=0.2),\n",
        "        transforms.RandomRotation(30),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406)*10, (0.229, 0.224, 0.225)*10),\n",
        "    ])\n",
        "\n",
        "    train = MyDataset('dataset', 'train', train_transforms)\n",
        "\n",
        "    train_size = int(0.7 * len(train))\n",
        "    valid_size = len(train)-train_size\n",
        "\n",
        "    train_dataset, val_dataset = random_split(train,[train_size,valid_size])\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, shuffle=True, batch_size=32, num_workers=8)\n",
        "    valid_loader = torch.utils.data.DataLoader(dataset=val_dataset,shuffle=False, batch_size=16,num_workers=4)\n",
        "\n",
        "    # step 2. load model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = get_model(model_name).to(device)\n",
        "\n",
        "    # step 3. prepare training tool\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
        "    # step 4. train model\n",
        "    model = MyModelWrapper_none(model_name, model=model, device=device, optimizer=optimizer, criterion=criterion)\n",
        "    # step 4. train model\n",
        "    model.fit(train_loader,valid_loader, nepoch=nepoch)\n",
        "def run_eff2(model_name='eff2', lr=1e-4, batch_size=32, nepoch=100): #pretrained 모델 있음\n",
        "\n",
        "    # step 1. load dataset\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.RandomCrop(224),\n",
        "        transforms.RandomHorizontalFlip(p=0.2),\n",
        "        transforms.RandomVerticalFlip(p=0.2),\n",
        "        transforms.RandomRotation(30),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406)*10, (0.229, 0.224, 0.225)*10),\n",
        "    ])\n",
        "    \n",
        "    train = MyDataset('dataset', 'train', train_transforms)\n",
        "    \n",
        "    train_size = int(0.7 * len(train))\n",
        "    valid_size = len(train)-train_size\n",
        "\n",
        "    train_dataset, val_dataset = random_split(train,[train_size,valid_size])\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, shuffle=True, batch_size=32, num_workers=8)\n",
        "    valid_loader = torch.utils.data.DataLoader(dataset=val_dataset,shuffle=False, batch_size=16,num_workers=4)\n",
        "    \n",
        "    # step 2. load model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = get_model(model_name).to(device)\n",
        "\n",
        "    # step 3. prepare training tool\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = RAdam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)    \n",
        "    model = MyModelWrapper(model_name, model=model, device=device, optimizer=optimizer, criterion=criterion, scheduler=scheduler)\n",
        "    model.fit(train_loader,valid_loader, nepoch=nepoch)\n",
        "\n",
        "def run_eff1(model_name='eff1', lr=1e-4, batch_size=32, nepoch=100): #pretrained 모델 있음\n",
        "\n",
        "    # step 1. load dataset\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.RandomCrop(224),\n",
        "        transforms.RandomHorizontalFlip(p=0.2),\n",
        "        transforms.RandomVerticalFlip(p=0.2),\n",
        "        transforms.RandomRotation(30),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406)*10, (0.229, 0.224, 0.225)*10),\n",
        "    ])\n",
        "    \n",
        "    train = MyDataset('dataset', 'train', train_transforms)\n",
        "    \n",
        "    train_size = int(0.7 * len(train))\n",
        "    valid_size = len(train)-train_size\n",
        "\n",
        "    train_dataset, val_dataset = random_split(train,[train_size,valid_size])\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, shuffle=True, batch_size=32, num_workers=8)\n",
        "    valid_loader = torch.utils.data.DataLoader(dataset=val_dataset,shuffle=False, batch_size=16,num_workers=4)\n",
        "    \n",
        "    # step 2. load model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = get_model(model_name).to(device)\n",
        "\n",
        "    # step 3. prepare training tool\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)    \n",
        "    model = MyModelWrapper(model_name, model=model, device=device, optimizer=optimizer, criterion=criterion, scheduler=scheduler)\n",
        "    model.fit(train_loader,valid_loader, nepoch=nepoch)\n",
        "\n",
        "def run_eff0(model_name='eff0', lr=1e-4, batch_size=32, nepoch=100): #pretrained 모델 있음\n",
        "\n",
        "    # step 1. load dataset\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.RandomCrop(224),\n",
        "        transforms.RandomHorizontalFlip(p=0.2),\n",
        "        transforms.RandomVerticalFlip(p=0.2),\n",
        "        transforms.RandomRotation(30),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406)*10, (0.229, 0.224, 0.225)*10),\n",
        "    ])\n",
        "    \n",
        "    train = MyDataset('dataset', 'train', train_transforms)\n",
        "    \n",
        "    train_size = int(0.7 * len(train))\n",
        "    valid_size = len(train)-train_size\n",
        "\n",
        "    train_dataset, val_dataset = random_split(train,[train_size,valid_size])\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, shuffle=True, batch_size=32, num_workers=8)\n",
        "    valid_loader = torch.utils.data.DataLoader(dataset=val_dataset,shuffle=False, batch_size=16,num_workers=4)\n",
        "    \n",
        "    # step 2. load model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = get_model(model_name).to(device)\n",
        "\n",
        "    # step 3. prepare training tool\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)    \n",
        "    model = MyModelWrapper(model_name, model=model, device=device, optimizer=optimizer, criterion=criterion, scheduler=scheduler)\n",
        "    model.fit(train_loader,valid_loader, nepoch=nepoch)"
      ],
      "id": "wKMvBchwI5Lc",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aACRutGfVqTl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8c9dbc48-9a36-4bd9-884d-008b755cc86a"
      },
      "source": [
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print('\\t'.join(entries))\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "def get_model(model_name, n_classes=50, pretrained=True):\n",
        "\n",
        "  if model_name==\"eff7\":\n",
        "    model = timm.create_model(\"efficientnet_b7\", pretrained=True, num_classes=n_classes)\n",
        "  elif model_name==\"eff6\":\n",
        "    model = timm.create_model(\"efficientnet_b6\", pretrained=True, num_classes=n_classes)\n",
        "  elif model_name==\"eff5\":\n",
        "    model = timm.create_model(\"efficientnet_b5\", pretrained=True, num_classes=n_classes)\n",
        "  elif model_name==\"eff4\":\n",
        "    model = timm.create_model(\"efficientnet_b4\", pretrained=True, num_classes=n_classes)\n",
        "  elif model_name==\"eff3\":\n",
        "    model = timm.create_model(\"efficientnet_b3\", pretrained=True, num_classes=n_classes)\n",
        "  elif model_name==\"eff2\":\n",
        "    model = timm.create_model(\"efficientnet_b2\", pretrained=True, num_classes=n_classes)\n",
        "  elif model_name==\"eff1\":\n",
        "    model = timm.create_model(\"efficientnet_b1\", pretrained=True, num_classes=n_classes)\n",
        "  elif model_name==\"eff0\":\n",
        "    model = timm.create_model(\"efficientnet_b0\", pretrained=True, num_classes=n_classes)\n",
        "  elif model_name==\"vit_large\":\n",
        "    model = timm.create_model(\"vit_large_patch16_224\", pretrained=True, num_classes=n_classes)\n",
        "  elif model_name==\"vit_base\":\n",
        "    model=timm.create_model(\"vit_base_patch16_224\", pretrained=True,num_classes=n_classes)  \n",
        "  elif model_name==\"ResNet_50\":\n",
        "    model = timm.create_model(\"resnet50\", pretrained=True, num_classes=n_classes)\n",
        "  elif model_name==\"ResNet_34\":\n",
        "    model = timm.create_model(\"resnet34\", pretrained=True, num_classes=n_classes)\n",
        "    \n",
        "  return MnistModel_efficientb7(model=model)\n",
        "\n",
        "\n",
        "\n",
        "def run_eff4(model_name='eff4', lr=1e-4, batch_size=32, nepoch=150): #pretrained 모델 있음\n",
        "    download_dataset()\n",
        "    # step 1. load dataset\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.RandomCrop(224),\n",
        "        transforms.RandomHorizontalFlip(p=0.2),\n",
        "        transforms.RandomVerticalFlip(p=0.2),\n",
        "        transforms.RandomRotation(30),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406)*10, (0.229, 0.224, 0.225)*10),\n",
        "    ])\n",
        "    \n",
        "    train = MyDataset('dataset', 'train', train_transforms)\n",
        "    \n",
        "    train_size = int(0.7 * len(train))\n",
        "    valid_size = len(train)-train_size\n",
        "\n",
        "    train_dataset, val_dataset = random_split(train,[train_size,valid_size])\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, shuffle=True, batch_size=32, num_workers=8)\n",
        "    valid_loader = torch.utils.data.DataLoader(dataset=val_dataset,shuffle=False, batch_size=16,num_workers=4)\n",
        "    \n",
        "    # step 2. load model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = get_model(model_name).to(device)\n",
        "\n",
        "    # step 3. prepare training tool\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = RAdam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=1e-4)     \n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)    \n",
        "    model = MyModelWrapper(model_name, model=model, device=device, optimizer=optimizer, criterion=criterion, scheduler=scheduler)\n",
        "    model.fit(train_loader,valid_loader, nepoch=nepoch)\n",
        "\n",
        "def run_eff3(model_name='eff3', lr=1e-4, batch_size=32, nepoch=100): #pretrained 모델 있음\n",
        "\n",
        "    # step 1. load dataset\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.RandomCrop(224),\n",
        "        transforms.RandomHorizontalFlip(p=0.2),\n",
        "        transforms.RandomVerticalFlip(p=0.2),\n",
        "        transforms.RandomRotation(30),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406)*10, (0.229, 0.224, 0.225)*10),\n",
        "    ])\n",
        "    \n",
        "    train = MyDataset('dataset', 'train', train_transforms)\n",
        "    \n",
        "    train_size = int(0.7 * len(train))\n",
        "    valid_size = len(train)-train_size\n",
        "\n",
        "    train_dataset, val_dataset = random_split(train,[train_size,valid_size])\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, shuffle=True, batch_size=32, num_workers=8)\n",
        "    valid_loader = torch.utils.data.DataLoader(dataset=val_dataset,shuffle=False, batch_size=16,num_workers=4)\n",
        "    \n",
        "    # step 2. load model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = get_model(model_name).to(device)\n",
        "\n",
        "    # step 3. prepare training tool\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = RAdam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=1e-4)    \n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)    \n",
        "    model = MyModelWrapper(model_name, model=model, device=device, optimizer=optimizer, criterion=criterion, scheduler=scheduler)\n",
        "    model.fit(train_loader,valid_loader, nepoch=nepoch)\n",
        "\n",
        "def run_rss_50(model_name='ResNet_50', lr=1e-5, batch_size=8, nepoch=100):\n",
        "    # step 1. load dataset\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.RandomCrop(224),\n",
        "        transforms.RandomHorizontalFlip(p=0.2),\n",
        "        transforms.RandomVerticalFlip(p=0.2),\n",
        "        transforms.RandomRotation(30),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406)*10, (0.229, 0.224, 0.225)*10),\n",
        "    ])\n",
        "\n",
        "    train = MyDataset('dataset', 'train', train_transforms)\n",
        "\n",
        "    train_size = int(0.7 * len(train))\n",
        "    valid_size = len(train)-train_size\n",
        "\n",
        "    train_dataset, val_dataset = random_split(train,[train_size,valid_size])\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, shuffle=True, batch_size=32, num_workers=8)\n",
        "    valid_loader = torch.utils.data.DataLoader(dataset=val_dataset,shuffle=False, batch_size=16,num_workers=4)\n",
        "\n",
        "    # step 2. load model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = get_model(model_name).to(device)\n",
        "\n",
        "    # step 3. prepare training tool\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = RAdam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=1e-4)    \n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)    \n",
        "    model = MyModelWrapper(model_name, model=model, device=device, optimizer=optimizer, criterion=criterion, scheduler=scheduler)\n",
        "\n",
        "    model.fit(train_loader,valid_loader, nepoch=nepoch)\n",
        "\n",
        "def run_rss_34(model_name='ResNet_34', lr=1e-5, batch_size=8, nepoch=150):\n",
        "    download_dataset()\n",
        "    # step 1. load dataset\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.RandomCrop(224),\n",
        "        transforms.RandomHorizontalFlip(p=0.2),\n",
        "        transforms.RandomVerticalFlip(p=0.2),\n",
        "        transforms.RandomRotation(30),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406)*10, (0.229, 0.224, 0.225)*10),\n",
        "    ])\n",
        "\n",
        "    train = MyDataset('dataset', 'train', train_transforms)\n",
        "\n",
        "    train_size = int(0.7 * len(train))\n",
        "    valid_size = len(train)-train_size\n",
        "\n",
        "    train_dataset, val_dataset = random_split(train,[train_size,valid_size])\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, shuffle=True, batch_size=32, num_workers=8)\n",
        "    valid_loader = torch.utils.data.DataLoader(dataset=val_dataset,shuffle=False, batch_size=16,num_workers=4)\n",
        "\n",
        "    # step 2. load model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = get_model(model_name).to(device)\n",
        "\n",
        "    # step 3. prepare training tool\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = RAdam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=1e-4)   \n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)    \n",
        "    model = MyModelWrapper(model_name, model=model, device=device, optimizer=optimizer, criterion=criterion, scheduler=scheduler)\n",
        "    model.fit(train_loader,valid_loader, nepoch=nepoch)\n",
        "\n",
        "#optimizer = torch.optim.AdamW(model.parameters(),lr=lr)\n",
        "#optimizer = RAdam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=1e-4)\n",
        "#optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
        "run_eff4()\n",
        "run_eff3()\n",
        "run_eff2()\n",
        "run_eff1()\n",
        "run_eff0()\n",
        "run_rss_50()\n",
        "run_rss_34()"
      ],
      "id": "aACRutGfVqTl",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [0][ 1/11]\tTime  0.734 ( 3.095)\tData  0.033 ( 2.356)\tLoss  4.1153 ( 4.0941)\tAcc@1   3.12 (  3.12)\tAcc@5   9.38 (  7.81)\n",
            "TRAIN: [0][ 2/11]\tTime  0.677 ( 2.289)\tData  0.041 ( 1.585)\tLoss  4.0152 ( 4.0678)\tAcc@1   0.00 (  2.08)\tAcc@5   6.25 (  7.29)\n",
            "TRAIN: [0][ 3/11]\tTime  0.644 ( 1.878)\tData  0.024 ( 1.195)\tLoss  3.9549 ( 4.0396)\tAcc@1   0.00 (  1.56)\tAcc@5  15.62 (  9.38)\n",
            "TRAIN: [0][ 4/11]\tTime  0.643 ( 1.631)\tData  0.022 ( 0.960)\tLoss  4.0059 ( 4.0328)\tAcc@1   6.25 (  2.50)\tAcc@5  12.50 ( 10.00)\n",
            "TRAIN: [0][ 5/11]\tTime  0.658 ( 1.469)\tData  0.022 ( 0.804)\tLoss  3.9539 ( 4.0197)\tAcc@1   3.12 (  2.60)\tAcc@5  12.50 ( 10.42)\n",
            "TRAIN: [0][ 6/11]\tTime  0.667 ( 1.354)\tData  0.022 ( 0.692)\tLoss  4.1674 ( 4.0408)\tAcc@1   3.12 (  2.68)\tAcc@5   3.12 (  9.38)\n",
            "TRAIN: [0][ 7/11]\tTime  0.662 ( 1.268)\tData  0.023 ( 0.608)\tLoss  4.1510 ( 4.0546)\tAcc@1   0.00 (  2.34)\tAcc@5   9.38 (  9.38)\n",
            "TRAIN: [0][ 8/11]\tTime  0.663 ( 1.201)\tData  0.025 ( 0.544)\tLoss  4.1341 ( 4.0634)\tAcc@1   0.00 (  2.08)\tAcc@5   6.25 (  9.03)\n",
            "TRAIN: [0][ 9/11]\tTime  0.662 ( 1.147)\tData  0.023 ( 0.491)\tLoss  3.9503 ( 4.0521)\tAcc@1   3.12 (  2.19)\tAcc@5  12.50 (  9.38)\n",
            "TRAIN: [0][10/11]\tTime  0.632 ( 1.100)\tData  0.023 ( 0.449)\tLoss  3.9348 ( 4.0420)\tAcc@1   0.00 (  2.00)\tAcc@5   6.67 (  9.14)\n",
            "TRAIN: [0][ 1/10]\tTime  0.644 ( 1.283)\tData  0.046 ( 0.644)\tLoss  3.7745 ( 3.9090)\tAcc@1   0.00 (  0.00)\tAcc@5   6.25 (  3.12)\n",
            "TRAIN: [0][ 2/10]\tTime  0.418 ( 0.994)\tData  0.015 ( 0.434)\tLoss  3.9149 ( 3.9110)\tAcc@1   0.00 (  0.00)\tAcc@5   6.25 (  4.17)\n",
            "TRAIN: [0][ 3/10]\tTime  0.402 ( 0.846)\tData  0.014 ( 0.329)\tLoss  4.0501 ( 3.9458)\tAcc@1   0.00 (  0.00)\tAcc@5   6.25 (  4.69)\n",
            "TRAIN: [0][ 4/10]\tTime  0.407 ( 0.758)\tData  0.013 ( 0.266)\tLoss  4.1088 ( 3.9784)\tAcc@1   6.25 (  1.25)\tAcc@5  12.50 (  6.25)\n",
            "TRAIN: [0][ 5/10]\tTime  0.408 ( 0.700)\tData  0.014 ( 0.224)\tLoss  3.9831 ( 3.9792)\tAcc@1   0.00 (  1.04)\tAcc@5   6.25 (  6.25)\n",
            "TRAIN: [0][ 6/10]\tTime  0.405 ( 0.658)\tData  0.014 ( 0.194)\tLoss  4.1527 ( 4.0040)\tAcc@1   0.00 (  0.89)\tAcc@5   0.00 (  5.36)\n",
            "TRAIN: [0][ 7/10]\tTime  0.408 ( 0.627)\tData  0.013 ( 0.171)\tLoss  3.9416 ( 3.9962)\tAcc@1   0.00 (  0.78)\tAcc@5   6.25 (  5.47)\n",
            "TRAIN: [0][ 8/10]\tTime  0.400 ( 0.602)\tData  0.014 ( 0.154)\tLoss  3.8337 ( 3.9781)\tAcc@1   6.25 (  1.39)\tAcc@5  25.00 (  7.64)\n",
            "TRAIN: [0][ 9/10]\tTime  0.254 ( 0.567)\tData  0.012 ( 0.140)\tLoss  4.1274 ( 3.9841)\tAcc@1   0.00 (  1.33)\tAcc@5   0.00 (  7.33)\n",
            "Saving best model(01.3333%) weight to log/best_weight/2021-09-29/12-14-26-eff4.pth\n",
            "Saving best loss model(03.9841%) weight to log/loss_weight/2021-09-29/12-14-26-eff4.pth\n",
            "======================================================================================================================================================\n",
            "[EPOCH]: 001/150    train loss 03.9841 acc 01.3333% (best accuracy : 01.3333 @ 001)\n",
            "======================================================================================================================================================\n",
            "TRAIN: [1][ 1/11]\tTime  0.765 ( 3.056)\tData  0.042 ( 2.162)\tLoss  4.0119 ( 4.0134)\tAcc@1   3.12 (  1.56)\tAcc@5   6.25 (  6.25)\n",
            "TRAIN: [1][ 2/11]\tTime  0.701 ( 2.271)\tData  0.036 ( 1.453)\tLoss  3.9841 ( 4.0037)\tAcc@1   3.12 (  2.08)\tAcc@5   9.38 (  7.29)\n",
            "TRAIN: [1][ 3/11]\tTime  0.665 ( 1.869)\tData  0.022 ( 1.096)\tLoss  4.0518 ( 4.0157)\tAcc@1   6.25 (  3.12)\tAcc@5   6.25 (  7.03)\n",
            "TRAIN: [1][ 4/11]\tTime  0.670 ( 1.629)\tData  0.022 ( 0.881)\tLoss  3.9949 ( 4.0115)\tAcc@1   0.00 (  2.50)\tAcc@5   3.12 (  6.25)\n",
            "TRAIN: [1][ 5/11]\tTime  0.668 ( 1.469)\tData  0.023 ( 0.738)\tLoss  4.1277 ( 4.0309)\tAcc@1   3.12 (  2.60)\tAcc@5   9.38 (  6.77)\n",
            "TRAIN: [1][ 6/11]\tTime  0.674 ( 1.356)\tData  0.024 ( 0.636)\tLoss  4.0809 ( 4.0380)\tAcc@1   0.00 (  2.23)\tAcc@5   3.12 (  6.25)\n",
            "TRAIN: [1][ 7/11]\tTime  0.669 ( 1.270)\tData  0.024 ( 0.559)\tLoss  4.0454 ( 4.0390)\tAcc@1   0.00 (  1.95)\tAcc@5   6.25 (  6.25)\n",
            "TRAIN: [1][ 8/11]\tTime  0.667 ( 1.203)\tData  0.025 ( 0.500)\tLoss  4.0326 ( 4.0382)\tAcc@1   0.00 (  1.74)\tAcc@5   9.38 (  6.60)\n",
            "TRAIN: [1][ 9/11]\tTime  0.668 ( 1.149)\tData  0.024 ( 0.452)\tLoss  3.8701 ( 4.0214)\tAcc@1   0.00 (  1.56)\tAcc@5   6.25 (  6.56)\n",
            "TRAIN: [1][10/11]\tTime  0.631 ( 1.102)\tData  0.021 ( 0.413)\tLoss  4.0845 ( 4.0268)\tAcc@1   3.33 (  1.71)\tAcc@5   6.67 (  6.57)\n",
            "TRAIN: [1][ 1/10]\tTime  0.520 ( 1.324)\tData  0.035 ( 0.672)\tLoss  3.8338 ( 3.8958)\tAcc@1   0.00 (  6.25)\tAcc@5   6.25 (  9.38)\n",
            "TRAIN: [1][ 2/10]\tTime  0.415 ( 1.021)\tData  0.019 ( 0.455)\tLoss  4.0139 ( 3.9352)\tAcc@1   0.00 (  4.17)\tAcc@5   0.00 (  6.25)\n",
            "TRAIN: [1][ 3/10]\tTime  0.411 ( 0.869)\tData  0.012 ( 0.344)\tLoss  3.8145 ( 3.9050)\tAcc@1   0.00 (  3.12)\tAcc@5  12.50 (  7.81)\n",
            "TRAIN: [1][ 4/10]\tTime  0.408 ( 0.776)\tData  0.016 ( 0.278)\tLoss  4.0107 ( 3.9262)\tAcc@1   6.25 (  3.75)\tAcc@5  18.75 ( 10.00)\n",
            "TRAIN: [1][ 5/10]\tTime  0.402 ( 0.714)\tData  0.012 ( 0.234)\tLoss  3.8890 ( 3.9200)\tAcc@1   0.00 (  3.12)\tAcc@5  18.75 ( 11.46)\n",
            "TRAIN: [1][ 6/10]\tTime  0.402 ( 0.670)\tData  0.012 ( 0.202)\tLoss  3.9168 ( 3.9195)\tAcc@1   6.25 (  3.57)\tAcc@5  18.75 ( 12.50)\n",
            "TRAIN: [1][ 7/10]\tTime  0.399 ( 0.636)\tData  0.012 ( 0.179)\tLoss  4.1238 ( 3.9451)\tAcc@1   0.00 (  3.12)\tAcc@5   0.00 ( 10.94)\n",
            "TRAIN: [1][ 8/10]\tTime  0.406 ( 0.610)\tData  0.015 ( 0.160)\tLoss  3.9331 ( 3.9437)\tAcc@1  12.50 (  4.17)\tAcc@5  18.75 ( 11.81)\n",
            "TRAIN: [1][ 9/10]\tTime  0.252 ( 0.574)\tData  0.012 ( 0.146)\tLoss  4.2704 ( 3.9568)\tAcc@1   0.00 (  4.00)\tAcc@5   0.00 ( 11.33)\n",
            "Saving best model(04.0000%) weight to log/best_weight/2021-09-29/12-14-26-eff4.pth\n",
            "Saving best loss model(03.9568%) weight to log/loss_weight/2021-09-29/12-14-26-eff4.pth\n",
            "======================================================================================================================================================\n",
            "[EPOCH]: 002/150    train loss 03.9568 acc 04.0000% (best accuracy : 04.0000 @ 002)\n",
            "======================================================================================================================================================\n",
            "TRAIN: [2][ 1/11]\tTime  0.755 ( 3.148)\tData  0.035 ( 2.344)\tLoss  3.7787 ( 3.8936)\tAcc@1   9.38 (  4.69)\tAcc@5  21.88 ( 14.06)\n",
            "TRAIN: [2][ 2/11]\tTime  0.676 ( 2.324)\tData  0.035 ( 1.574)\tLoss  4.0580 ( 3.9484)\tAcc@1   6.25 (  5.21)\tAcc@5   6.25 ( 11.46)\n",
            "TRAIN: [2][ 3/11]\tTime  0.662 ( 1.908)\tData  0.024 ( 1.187)\tLoss  4.0383 ( 3.9709)\tAcc@1   0.00 (  3.91)\tAcc@5  12.50 ( 11.72)\n",
            "TRAIN: [2][ 4/11]\tTime  0.665 ( 1.660)\tData  0.024 ( 0.954)\tLoss  4.0202 ( 3.9807)\tAcc@1   0.00 (  3.12)\tAcc@5   0.00 (  9.38)\n",
            "TRAIN: [2][ 5/11]\tTime  0.664 ( 1.494)\tData  0.026 ( 0.800)\tLoss  3.9221 ( 3.9710)\tAcc@1   0.00 (  2.60)\tAcc@5   9.38 (  9.38)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-694fdcec5616>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;31m#optimizer = RAdam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=1e-4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;31m#optimizer = torch.optim.Adam(model.parameters(),lr=lr)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m \u001b[0mrun_eff4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0mrun_eff3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0mrun_eff2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-694fdcec5616>\u001b[0m in \u001b[0;36mrun_eff4\u001b[0;34m(model_name, lr, batch_size, nepoch)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCosineAnnealingLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta_min\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyModelWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_eff3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'eff3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#pretrained 모델 있음\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-24a719f98ff3>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_dl, valid_dl, nepoch)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-24a719f98ff3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dl, epoch)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mtop5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc5\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7lDfs6ZOlIw"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406)*10, (0.229, 0.224, 0.225)*10),\n",
        "])\n",
        "\n",
        "test = MyDataset('dataset', 'test', test_transforms)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test, shuffle=False, batch_size=1, num_workers=1)"
      ],
      "id": "I7lDfs6ZOlIw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWVs3_tp15bt"
      },
      "source": [
        "Efficient_acc_eff1_1R = get_model('eff1')\n",
        "Efficient_acc_eff1_1R.load_state_dict(torch.load('/content/drive/MyDrive/model/06-15-05-eff1.pth')['weight'])\n",
        "Efficient_acc_eff1_1R.to(device)\n",
        "Efficient_acc_eff1_1R.eval()\n",
        "\n",
        "Efficient_acc_eff0_1R = get_model('eff0')\n",
        "Efficient_acc_eff0_1R.load_state_dict(torch.load('/content/drive/MyDrive/model/06-30-31-eff0.pth')['weight'])\n",
        "Efficient_acc_eff0_1R.to(device)\n",
        "Efficient_acc_eff0_1R.eval()\n",
        "\n",
        "Efficient_acc_eff2_1R = get_model('eff2')\n",
        "Efficient_acc_eff2_1R.load_state_dict(torch.load('/content/drive/MyDrive/model/06-23-08-eff2.pth')['weight'])\n",
        "Efficient_acc_eff2_1R.to(device)\n",
        "Efficient_acc_eff2_1R.eval()\n",
        "\n",
        "\n",
        "Efficient_acc_eff1_2R = get_model('eff1')\n",
        "Efficient_acc_eff1_2R.load_state_dict(torch.load('/content/drive/MyDrive/model/07-00-22-eff1.pth')['weight'])\n",
        "Efficient_acc_eff1_2R.to(device)\n",
        "Efficient_acc_eff1_2R.eval()\n",
        "\n",
        "Efficient_acc_eff0_2R = get_model('eff0')\n",
        "Efficient_acc_eff0_2R.load_state_dict(torch.load('/content/drive/MyDrive/model/07-08-08-eff0 (1).pth')['weight'])\n",
        "Efficient_acc_eff0_2R.to(device)\n",
        "Efficient_acc_eff0_2R.eval()\n",
        "\n",
        "\n",
        "Efficient_acc_eff2_2R = get_model('eff2')\n",
        "Efficient_acc_eff2_2R.load_state_dict(torch.load('/content/drive/MyDrive/model/06-52-58-eff2.pth')['weight'])\n",
        "Efficient_acc_eff2_2R.to(device)\n",
        "Efficient_acc_eff2_2R.eval()\n",
        "\n",
        "Efficient_acc_eff2_3R = get_model('eff2')\n",
        "Efficient_acc_eff2_3R.load_state_dict(torch.load('/content/drive/MyDrive/model/07-15-18-eff2.pth')['weight'])\n",
        "Efficient_acc_eff2_3R.to(device)\n",
        "Efficient_acc_eff2_3R.eval()\n",
        "\n",
        "Efficient_acc_eff1_3R = get_model('eff1')\n",
        "Efficient_acc_eff1_3R.load_state_dict(torch.load('/content/drive/MyDrive/model/07-34-43-eff1.pth')['weight'])\n",
        "Efficient_acc_eff1_3R.to(device)\n",
        "Efficient_acc_eff1_3R.eval()\n",
        "\n",
        "Efficient_acc_eff0_3R = get_model('eff0')\n",
        "Efficient_acc_eff0_3R.load_state_dict(torch.load('/content/drive/MyDrive/model/07-42-20-eff0.pth')['weight'])\n",
        "Efficient_acc_eff0_3R.to(device)\n",
        "Efficient_acc_eff0_3R.eval()\n",
        "\n",
        "Efficient_acc_eff2_4R = get_model('eff2')\n",
        "Efficient_acc_eff2_4R.load_state_dict(torch.load('/content/drive/MyDrive/model/07-48-29-eff2.pth')['weight'])\n",
        "Efficient_acc_eff2_4R.to(device)\n",
        "Efficient_acc_eff2_4R.eval()\n",
        "\n",
        "\n",
        "Efficient_acc_eff0_4R = get_model('eff0')\n",
        "Efficient_acc_eff0_4R.load_state_dict(torch.load('/content/drive/MyDrive/model/08-14-56-eff0.pth')['weight'])\n",
        "Efficient_acc_eff0_4R.to(device)\n",
        "Efficient_acc_eff0_4R.eval()\n",
        "\n",
        "\n",
        "Efficient_acc_eff4_2R = get_model('eff4')\n",
        "Efficient_acc_eff4_2R.load_state_dict(torch.load('/content/drive/MyDrive/model/00-20-13-eff4.pth')['weight'])\n",
        "Efficient_acc_eff4_2R.to(device)\n",
        "Efficient_acc_eff4_2R.eval()\n",
        "\n",
        "\n",
        "Efficient_acc_eff3_2R = get_model('eff3')\n",
        "Efficient_acc_eff3_2R.load_state_dict(torch.load('/content/drive/MyDrive/model/00-36-21-eff3.pth')['weight'])\n",
        "Efficient_acc_eff3_2R.to(device)\n",
        "Efficient_acc_eff3_2R.eval()\n",
        "\n",
        "ResNet_50_acc_2R = get_model('ResNet_50')\n",
        "ResNet_50_acc_2R.load_state_dict(torch.load('/content/drive/MyDrive/model/00-48-17-ResNet_50.pth')['weight'])\n",
        "ResNet_50_acc_2R.to(device)\n",
        "ResNet_50_acc_2R.eval()\n",
        "\n",
        "\n",
        "ResNet_34_acc_2R = get_model('ResNet_34')\n",
        "ResNet_34_acc_2R.load_state_dict(torch.load('/content/drive/MyDrive/model/01-09-42-ResNet_34.pth')['weight'])\n",
        "ResNet_34_acc_2R.to(device)\n",
        "ResNet_34_acc_2R.eval()\n",
        "\n",
        "Efficient_acc_eff4_1R = get_model('eff4')\n",
        "Efficient_acc_eff4_1R.load_state_dict(torch.load('/content/drive/MyDrive/model/01-29-24-eff4.pth')['weight'])\n",
        "Efficient_acc_eff4_1R.to(device)\n",
        "Efficient_acc_eff4_1R.eval()\n",
        "\n",
        "\n",
        "Efficient_acc_eff3_1R = get_model('eff3')\n",
        "Efficient_acc_eff3_1R.load_state_dict(torch.load('/content/drive/MyDrive/model/01-48-57-eff3.pth')['weight'])\n",
        "Efficient_acc_eff3_1R.to(device)\n",
        "Efficient_acc_eff3_1R.eval()\n",
        "\n",
        "\n",
        "ResNet_50_acc_1R = get_model('ResNet_50')\n",
        "ResNet_50_acc_1R.load_state_dict(torch.load('/content/drive/MyDrive/model/01-56-48-ResNet_50.pth')['weight'])\n",
        "ResNet_50_acc_1R.to(device)\n",
        "ResNet_50_acc_1R.eval()\n",
        "\n",
        "ResNet_34_acc_1R = get_model('ResNet_34')\n",
        "ResNet_34_acc_1R.load_state_dict(torch.load('/content/drive/MyDrive/model/02-08-10-ResNet_34.pth')['weight'])\n",
        "ResNet_34_acc_1R.to(device)\n",
        "ResNet_34_acc_1R.eval()\n",
        "\n",
        "Efficient_acc_eff4_3R = get_model('eff4')\n",
        "Efficient_acc_eff4_3R.load_state_dict(torch.load('/content/drive/MyDrive/model/02-19-48-eff4.pth')['weight'])\n",
        "Efficient_acc_eff4_3R.to(device)\n",
        "Efficient_acc_eff4_3R.eval()\n",
        "\n",
        "\n",
        "Efficient_acc_eff3_3R = get_model('eff3')\n",
        "Efficient_acc_eff3_3R.load_state_dict(torch.load('/content/drive/MyDrive/model/02-28-38-eff3.pth')['weight'])\n",
        "Efficient_acc_eff3_3R.to(device)\n",
        "Efficient_acc_eff3_3R.eval()\n",
        "\n",
        "ResNet_50_acc_3R = get_model('ResNet_50')\n",
        "ResNet_50_acc_3R.load_state_dict(torch.load('/content/drive/MyDrive/model/02-37-47-ResNet_50.pth')['weight'])\n",
        "ResNet_50_acc_3R.to(device)\n",
        "ResNet_50_acc_3R.eval()\n",
        "\n",
        "\n",
        "ResNet_34_acc_3R = get_model('ResNet_34')\n",
        "ResNet_34_acc_3R.load_state_dict(torch.load('/content/drive/MyDrive/model/02-59-59-ResNet_34.pth')['weight'])\n",
        "ResNet_34_acc_3R.to(device)\n",
        "ResNet_34_acc_3R.eval()\n",
        "\n",
        "\n",
        "Efficient_acc_eff4_4R = get_model('eff4')\n",
        "Efficient_acc_eff4_4R.load_state_dict(torch.load('/content/drive/MyDrive/model/03-22-02-eff4.pth')['weight'])\n",
        "Efficient_acc_eff4_4R.to(device)\n",
        "Efficient_acc_eff4_4R.eval()\n",
        "\n",
        "\n",
        "Efficient_acc_eff3_4R = get_model('eff3')\n",
        "Efficient_acc_eff3_4R.load_state_dict(torch.load('/content/drive/MyDrive/model/03-44-21-eff3.pth')['weight'])\n",
        "Efficient_acc_eff3_4R.to(device)\n",
        "Efficient_acc_eff3_4R.eval()\n",
        "\n",
        "ResNet_50_acc_4R = get_model('ResNet_50')\n",
        "ResNet_50_acc_4R.load_state_dict(torch.load('/content/drive/MyDrive/model/04-07-31-ResNet_50.pth')['weight'])\n",
        "ResNet_50_acc_4R.to(device)\n",
        "ResNet_50_acc_4R.eval()\n",
        "\n",
        "\n",
        "ResNet_34_acc_4R = get_model('ResNet_34')\n",
        "ResNet_34_acc_4R.load_state_dict(torch.load('/content/drive/MyDrive/model/04-26-46-ResNet_34.pth')['weight'])\n",
        "ResNet_34_acc_4R.to(device)\n",
        "ResNet_34_acc_4R.eval()\n",
        "\n",
        "\n",
        "batch_size=1\n",
        "\n",
        "def Ensemble():\n",
        "  for idx,data in enumerate(test_loader):\n",
        "    images, labels = data\n",
        "    images, labels = images.cuda(), labels.cuda()\n",
        "\n",
        "    outputs = Efficient_acc_eff2_1R(images)\n",
        "    outputs += Efficient_acc_eff1_1R(images)\n",
        "    outputs += Efficient_acc_eff2_2R(images)\n",
        "    outputs += Efficient_acc_eff1_2R(images)\n",
        "    outputs += Efficient_acc_eff2_3R(images)\n",
        "    outputs += Efficient_acc_eff1_3R(images)\n",
        "    outputs += Efficient_acc_eff2_4R(images)\n",
        "\n",
        "    outputs += Efficient_acc_eff4_2R(images)\n",
        "    outputs += Efficient_acc_eff3_2R(images)\n",
        "    outputs += ResNet_50_acc_2R(images)\n",
        "\n",
        "    outputs += Efficient_acc_eff4_1R(images)\n",
        "    outputs += Efficient_acc_eff3_1R(images)\n",
        "\n",
        "    outputs += Efficient_acc_eff4_3R(images)\n",
        "    outputs += Efficient_acc_eff3_3R(images)\n",
        "\n",
        "    outputs += Efficient_acc_eff4_4R(images)\n",
        "    outputs += Efficient_acc_eff3_4R(images)\n",
        "    outputs += ResNet_50_acc_4R(images)\n",
        "\n",
        "\n",
        "    _, predictions = torch.max(outputs, 1)\n",
        "    predictions= torch.reshape(predictions,(-1,1))\n",
        "    batch_index=batch_size*idx\n",
        "    probs_array[batch_index:batch_index+batch_size]= predictions.cpu()\n",
        "  return probs_array\n",
        "\n",
        "sample_submission = pd.read_csv(\"https://raw.githubusercontent.com/cjfghk5697/AjouDeeplearning_Compete/main/sample_submission.csv\")\n",
        "probs_array = np.zeros([sample_submission.shape[0],\n",
        "                              sample_submission.shape[1] -1])\n",
        "probs_array=Ensemble()\n",
        "sample_submission.iloc[:,1:] = probs_array\n",
        "sample_submission=sample_submission.astype(int)\n",
        "sample_submission.to_csv('efficientNet_Ensemble.csv',index = False) # 경로 수정 필요 \n"
      ],
      "id": "JWVs3_tp15bt",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gn7pQpaDsa1y"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "Gn7pQpaDsa1y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUw8iLkx6YYm"
      },
      "source": [
        "# activation:     optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
        "\n",
        "# / learning scheduler: scheduler = CosineAnnealingWarmupRestarts(optimizer,\n",
        "#                                          first_cycle_steps=60,\n",
        "#                                         cycle_mult=1.0,\n",
        "#                                          max_lr=0.005,\n",
        "#                                          min_lr=0.0001,\n",
        "#                                          warmup_steps=12,\n",
        "#                                          gamma=1.0)\n",
        "/content/drive/MyDrive/model/07-42-20-eff0.pth\n",
        "\n",
        "Efficient_acc_eff4_2A = get_model('eff4')\n",
        "Efficient_acc_eff4_2A.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/12-52-55-eff4.pth')['weight'])\n",
        "Efficient_acc_eff4_2A.to(device)\n",
        "Efficient_acc_eff4_2A.eval()\n",
        "\n",
        "\n",
        "Efficient_acc_eff3_2A = get_model('eff3')\n",
        "Efficient_acc_eff3_2A.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/12-59-11-eff3.pth')['weight'])\n",
        "Efficient_acc_eff3_2A.to(device)\n",
        "Efficient_acc_eff3_2A.eval()\n",
        "\n",
        "Efficient_acc_eff2_2A = get_model('eff2')\n",
        "Efficient_acc_eff2_2A.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/12-52-55-eff4.pth')['weight'])\n",
        "Efficient_acc_eff2_2A.to(device)\n",
        "Efficient_acc_eff2_2A.eval()\n",
        "\n",
        "\n",
        "Efficient_acc_eff1_2A = get_model('eff1')\n",
        "Efficient_acc_eff1_2A.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/12-59-11-eff3.pth')['weight'])\n",
        "Efficient_acc_eff1_2A.to(device)\n",
        "Efficient_acc_eff1_2A.eval()\n",
        "\n",
        "Efficient_acc_eff0_2A = get_model('eff0')\n",
        "Efficient_acc_eff0_2A.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/12-59-11-eff3.pth')['weight'])\n",
        "Efficient_acc_eff0_2A.to(device)\n",
        "Efficient_acc_eff0_2A.eval()\n",
        "\n",
        "ResNet_50_acc_2A = get_model('ResNet_50')\n",
        "ResNet_50_acc_2A.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/13-09-54-ResNet_50.pth')['weight'])\n",
        "ResNet_50_acc_2A.to(device)\n",
        "ResNet_50_acc_2A.eval()\n",
        "\n",
        "\n",
        "ResNet_34_acc_2A = get_model('ResNet_34')\n",
        "ResNet_34_acc_2A.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/13-04-02-ResNet_34.pth')['weight'])\n",
        "ResNet_34_acc_2A.to(device)\n",
        "ResNet_34_acc_2A.eval()\n",
        "\n",
        "# activation: Adam / learning       \n",
        "#    decay_steps = (len(train_dataset)//batch_size)*nepoch\n",
        "#    scheduler = PolynomialLRDecay(optimizer, max_decay_steps=decay_steps, end_learning_rate=1e-6, power=0.9)   \n",
        "\n",
        "\n",
        "\n",
        "Efficient_acc_eff4_1A = get_model('eff4')\n",
        "Efficient_acc_eff4_1A.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/13-46-17-eff4.pth')['weight'])\n",
        "Efficient_acc_eff4_1A.to(device)\n",
        "Efficient_acc_eff4_1A.eval()\n",
        "\n",
        "Efficient_acc_eff3_1A = get_model('eff3')\n",
        "Efficient_acc_eff3_1A.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/13-55-19-eff3.pth')['weight'])\n",
        "Efficient_acc_eff3_1A.to(device)\n",
        "Efficient_acc_eff3_1A.eval()\n",
        "\n",
        "Efficient_acc_eff2_1A = get_model('eff2')\n",
        "Efficient_acc_eff2_1A.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/12-52-55-eff4.pth')['weight'])\n",
        "Efficient_acc_eff2_1A.to(device)\n",
        "Efficient_acc_eff2_1A.eval()\n",
        "\n",
        "\n",
        "Efficient_acc_eff1_1A = get_model('eff1')\n",
        "Efficient_acc_eff1_1A.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/12-59-11-eff3.pth')['weight'])\n",
        "Efficient_acc_eff1_1A.to(device)\n",
        "Efficient_acc_eff1_1A.eval()\n",
        "\n",
        "Efficient_acc_eff0_1A = get_model('eff0')\n",
        "Efficient_acc_eff0_1A.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/12-59-11-eff3.pth')['weight'])\n",
        "Efficient_acc_eff0_1A.to(device)\n",
        "Efficient_acc_eff0_1A.eval()\n",
        "\n",
        "\n",
        "Vit_base_acc_1A=get_model('vit_base')\n",
        "Vit_base_acc_1A.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/19-19-06-vit_base.pth')['weight'])\n",
        "Vit_base_acc_1A.to(device)\n",
        "Vit_base_acc_1A.eval()\n",
        "\n",
        "ResNet_50_acc_1A = get_model('ResNet_50')\n",
        "ResNet_50_acc_1A.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/13-58-57-ResNet_50.pth')['weight'])\n",
        "ResNet_50_acc_1A.to(device)\n",
        "ResNet_50_acc_1A.eval()\n",
        "\n",
        "ResNet_34_acc_1A = get_model('ResNet_34')\n",
        "ResNet_34_acc_1A.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/14-09-17-ResNet_34.pth')['weight'])\n",
        "ResNet_34_acc_1A.to(device)\n",
        "ResNet_34_acc_1A.eval()\n",
        "\n",
        "\n",
        "# Optimizer: Adam learning scheduler:  scheduler =  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\n",
        "\n",
        "Efficient_acc_eff4_3A = get_model('eff4')\n",
        "Efficient_acc_eff4_3A.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/14-21-05-eff4.pth')['weight'])\n",
        "Efficient_acc_eff4_3A.to(device)\n",
        "Efficient_acc_eff4_3A.eval()\n",
        "\n",
        "Efficient_acc_eff3_3A = get_model('eff3')\n",
        "Efficient_acc_eff3_3A.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/14-30-48-eff3.pth')['weight'])\n",
        "Efficient_acc_eff3_3A.to(device)\n",
        "Efficient_acc_eff3_3A.eval()\n",
        "\n",
        "Efficient_acc_eff2_3A = get_model('eff2')\n",
        "Efficient_acc_eff2_3A.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/12-52-55-eff4.pth')['weight'])\n",
        "Efficient_acc_eff2_3A.to(device)\n",
        "Efficient_acc_eff2_3A.eval()\n",
        "\n",
        "\n",
        "Efficient_acc_eff1_3A = get_model('eff1')\n",
        "Efficient_acc_eff1_3A.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/12-59-11-eff3.pth')['weight'])\n",
        "Efficient_acc_eff1_3A.to(device)\n",
        "Efficient_acc_eff1_3A.eval()\n",
        "\n",
        "Efficient_acc_eff0_3A = get_model('eff0')\n",
        "Efficient_acc_eff0_3A.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/12-59-11-eff3.pth')['weight'])\n",
        "Efficient_acc_eff0_3A.to(device)\n",
        "Efficient_acc_eff0_3A.eval()\n",
        "\n",
        "Vit_base_acc_3A=get_model('vit_base')\n",
        "Vit_base_acc_3A.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/20-14-38-vit_base.pth')['weight'])\n",
        "Vit_base_acc_3A.to(device)\n",
        "Vit_base_acc_3A.eval()\n",
        "\n",
        "ResNet_50_acc_3A = get_model('ResNet_50')\n",
        "ResNet_50_acc_3A.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/14-34-36-ResNet_50.pth')['weight'])\n",
        "ResNet_50_acc_3A.to(device)\n",
        "ResNet_50_acc_3A.eval()\n",
        "\n",
        "ResNet_34_acc_3A = get_model('ResNet_34')\n",
        "ResNet_34_acc_3A.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/14-44-32-ResNet_34.pth')['weight'])\n",
        "ResNet_34_acc_3A.to(device)\n",
        "ResNet_34_acc_3A.eval()"
      ],
      "id": "VUw8iLkx6YYm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCCF_1zyI3-7"
      },
      "source": [
        "# optimizer = RAdam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=1e-4)\n",
        "# learning scheduler: scheduler = CosineAnnealingWarmupRestarts(optimizer,\n",
        "#                                          first_cycle_steps=60,\n",
        "#                                         cycle_mult=1.0,\n",
        "#                                          max_lr=0.005,\n",
        "#                                          min_lr=0.0001,\n",
        "#                                          warmup_steps=12,\n",
        "#                                          gamma=1.0)\n",
        "Efficient_acc_eff4_2R = get_model('eff4')\n",
        "Efficient_acc_eff4_2R.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/14-54-33-eff4.pth')['weight'])\n",
        "Efficient_acc_eff4_2R.to(device)\n",
        "Efficient_acc_eff4_2R.eval()\n",
        "\n",
        "Efficient_acc_eff3_2R = get_model('eff3')\n",
        "Efficient_acc_eff3_2R.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/14-58-23-eff3.pth')['weight'])\n",
        "Efficient_acc_eff3_2R.to(device)\n",
        "Efficient_acc_eff3_2R.eval()\n",
        "\n",
        "Efficient_acc_eff2_2R = get_model('eff2')\n",
        "Efficient_acc_eff2_2R.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/12-52-55-eff4.pth')['weight'])\n",
        "Efficient_acc_eff2_2R.to(device)\n",
        "Efficient_acc_eff2_2R.eval()\n",
        "\n",
        "\n",
        "Efficient_acc_eff1_2R = get_model('eff1')\n",
        "Efficient_acc_eff1_2R.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/12-59-11-eff3.pth')['weight'])\n",
        "Efficient_acc_eff1_2R.to(device)\n",
        "Efficient_acc_eff1_2R.eval()\n",
        "\n",
        "Efficient_acc_eff0_2R = get_model('eff0')\n",
        "Efficient_acc_eff0_2R.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/12-59-11-eff3.pth')['weight'])\n",
        "Efficient_acc_eff0_2R.to(device)\n",
        "Efficient_acc_eff0_2R.eval()\n",
        "\n",
        "\n",
        "ResNet_50_acc_2R = get_model('ResNet_50')\n",
        "ResNet_50_acc_2R.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/15-00-58-ResNet_50.pth')['weight'])\n",
        "ResNet_50_acc_2R.to(device)\n",
        "ResNet_50_acc_2R.eval()\n",
        "\n",
        "ResNet_34_acc_2R = get_model('ResNet_34')\n",
        "ResNet_34_acc_2R.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/15-08-28-ResNet_34.pth')['weight'])\n",
        "ResNet_34_acc_2R.to(device)\n",
        "ResNet_34_acc_2R.eval()\n",
        "\n",
        "# optimizer = RAdam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=1e-4)\n",
        "#    decay_steps = (len(train_dataset)//batch_size)*nepoch\n",
        "#    scheduler = PolynomialLRDecay(optimizer, max_decay_steps=decay_steps, end_learning_rate=1e-6, power=0.9)   \n",
        "\n",
        "Efficient_acc_eff4_1R = get_model('eff4')\n",
        "Efficient_acc_eff4_1R.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/15-15-04-eff4.pth')['weight'])\n",
        "Efficient_acc_eff4_1R.to(device)\n",
        "Efficient_acc_eff4_1R.eval()\n",
        "\n",
        "Efficient_acc_eff3_1R = get_model('eff3')\n",
        "Efficient_acc_eff3_1R.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/15-30-07-eff3.pth')['weight'])\n",
        "Efficient_acc_eff3_1R.to(device)\n",
        "Efficient_acc_eff3_1R.eval()\n",
        "\n",
        "Efficient_acc_eff2_1R = get_model('eff2')\n",
        "Efficient_acc_eff2_1R.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/12-52-55-eff4.pth')['weight'])\n",
        "Efficient_acc_eff2_1R.to(device)\n",
        "Efficient_acc_eff2_1R.eval()\n",
        "\n",
        "\n",
        "Efficient_acc_eff1_1R = get_model('eff1')\n",
        "Efficient_acc_eff1_1R.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/12-59-11-eff3.pth')['weight'])\n",
        "Efficient_acc_eff1_1R.to(device)\n",
        "Efficient_acc_eff1_1R.eval()\n",
        "\n",
        "Efficient_acc_eff0_1R = get_model('eff0')\n",
        "Efficient_acc_eff0_1R.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/12-59-11-eff3.pth')['weight'])\n",
        "Efficient_acc_eff0_1R.to(device)\n",
        "Efficient_acc_eff0_1R.eval()\n",
        "\n",
        "\n",
        "Vit_base_acc_1R=get_model('vit_base')\n",
        "Vit_base_acc_1R.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/19-29-02-vit_base.pth')['weight'])\n",
        "Vit_base_acc_1R.to(device)\n",
        "Vit_base_acc_1R.eval()\n",
        "\n",
        "ResNet_50_acc_1R = get_model('ResNet_50')\n",
        "ResNet_50_acc_1R.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/15-39-47-ResNet_50.pth')['weight'])\n",
        "ResNet_50_acc_1R.to(device)\n",
        "ResNet_50_acc_1R.eval()\n",
        "\n",
        "ResNet_34_acc_1R = get_model('ResNet_34')\n",
        "ResNet_34_acc_1R.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/15-59-19-ResNet_34.pth')['weight'])\n",
        "ResNet_34_acc_1R.to(device)\n",
        "ResNet_34_acc_1R.eval()\n",
        "\n",
        "# optimizer = RAdam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=1e-4)\n",
        "  \n",
        "Efficient_acc_eff4_3R = get_model('eff4')\n",
        "Efficient_acc_eff4_3R.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/16-17-34-eff4.pth')['weight'])\n",
        "Efficient_acc_eff4_3R.to(device)\n",
        "Efficient_acc_eff4_3R.eval()\n",
        "\n",
        "Efficient_acc_eff3_3R = get_model('eff3')\n",
        "Efficient_acc_eff3_3R.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/16-33-59-eff3.pth')['weight'])\n",
        "Efficient_acc_eff3_3R.to(device)\n",
        "Efficient_acc_eff3_3R.eval()\n",
        "\n",
        "Efficient_acc_eff2_3R = get_model('eff2')\n",
        "Efficient_acc_eff2_3R.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/12-52-55-eff4.pth')['weight'])\n",
        "Efficient_acc_eff2_3R.to(device)\n",
        "Efficient_acc_eff2_3R.eval()\n",
        "\n",
        "\n",
        "Efficient_acc_eff1_3R = get_model('eff1')\n",
        "Efficient_acc_eff1_3R.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/12-59-11-eff3.pth')['weight'])\n",
        "Efficient_acc_eff1_3R.to(device)\n",
        "Efficient_acc_eff1_3R.eval()\n",
        "\n",
        "Efficient_acc_eff0_3R = get_model('eff0')\n",
        "Efficient_acc_eff0_3R.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/12-59-11-eff3.pth')['weight'])\n",
        "Efficient_acc_eff0_3R.to(device)\n",
        "Efficient_acc_eff0_3R.eval()\n",
        "\n",
        "Vit_base_acc_3R=get_model('vit_base')\n",
        "Vit_base_acc_3R.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/20-10-19-vit_base.pth')['weight'])\n",
        "Vit_base_acc_3R.to(device)\n",
        "Vit_base_acc_3R.eval()\n",
        "\n",
        "ResNet_50_acc_3R = get_model('ResNet_50')\n",
        "ResNet_50_acc_3R.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/16-43-55-ResNet_50.pth')['weight'])\n",
        "ResNet_50_acc_3R.to(device)\n",
        "ResNet_50_acc_3R.eval()\n",
        "\n",
        "ResNet_34_acc_3R = get_model('ResNet_34')\n",
        "ResNet_34_acc_3R.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/17-02-47-ResNet_34.pth')['weight'])\n",
        "ResNet_34_acc_3R.to(device)\n",
        "ResNet_34_acc_3R.eval()"
      ],
      "id": "CCCF_1zyI3-7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykHGMiOatS6C"
      },
      "source": [
        "# activation:AdamW     optimizer = torch.optim.AdamW(model.parameters(),lr=lr)\n",
        "# learning scheduler: scheduler = CosineAnnealingWarmupRestarts(optimizer,\n",
        "#                                          first_cycle_steps=60,\n",
        "#                                         cycle_mult=1.0,\n",
        "#                                          max_lr=0.005,\n",
        "#                                          min_lr=0.0001,\n",
        "#                                          warmup_steps=12,\n",
        "#                                          gamma=1.0)\n",
        "\n",
        "Efficient_acc_eff4_2W = get_model('eff4')\n",
        "Efficient_acc_eff4_2W.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/17-26-55-eff4.pth')['weight'])\n",
        "Efficient_acc_eff4_2W.to(device)\n",
        "Efficient_acc_eff4_2W.eval()\n",
        "\n",
        "Efficient_acc_eff3_2W = get_model('eff3')\n",
        "Efficient_acc_eff3_2W.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/17-30-02-eff3.pth')['weight'])\n",
        "Efficient_acc_eff3_2W.to(device)\n",
        "Efficient_acc_eff3_2W.eval()\n",
        "\n",
        "Efficient_acc_eff2_2W = get_model('eff2')\n",
        "Efficient_acc_eff2_2W.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/12-52-55-eff4.pth')['weight'])\n",
        "Efficient_acc_eff2_2W.to(device)\n",
        "Efficient_acc_eff2_2W.eval()\n",
        "\n",
        "\n",
        "Efficient_acc_eff1_2W = get_model('eff1')\n",
        "Efficient_acc_eff1_2W.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/12-59-11-eff3.pth')['weight'])\n",
        "Efficient_acc_eff1_2W.to(device)\n",
        "Efficient_acc_eff1_2W.eval()\n",
        "\n",
        "Efficient_acc_eff0_2W = get_model('eff0')\n",
        "Efficient_acc_eff0_2W.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/12-59-11-eff3.pth')['weight'])\n",
        "Efficient_acc_eff0_2W.to(device)\n",
        "Efficient_acc_eff0_2W.eval()\n",
        "\n",
        "\n",
        "ResNet_50_acc_2W = get_model('ResNet_50')\n",
        "ResNet_50_acc_2W.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/17-35-06-ResNet_50.pth')['weight'])\n",
        "ResNet_50_acc_2W.to(device)\n",
        "ResNet_50_acc_2W.eval()\n",
        "\n",
        "ResNet_34_acc_2W = get_model('ResNet_34')\n",
        "ResNet_34_acc_2W.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/17-44-26-ResNet_34.pth')['weight'])\n",
        "ResNet_34_acc_2W.to(device)\n",
        "ResNet_34_acc_2W.eval()\n",
        "\n",
        "# activation: AdamW / learning scheduler\n",
        "#    decay_steps = (len(train_dataset)//batch_size)*nepoch\n",
        "#    scheduler = PolynomialLRDecay(optimizer, max_decay_steps=decay_steps, end_learning_rate=1e-6, power=0.9)   \n",
        "\n",
        "Efficient_acc_eff4_1W = get_model('eff4')\n",
        "Efficient_acc_eff4_1W.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/17-50-30-eff4.pth')['weight'])\n",
        "Efficient_acc_eff4_1W.to(device)\n",
        "Efficient_acc_eff4_1W.eval()\n",
        "\n",
        "Efficient_acc_eff3_1W = get_model('eff3')\n",
        "Efficient_acc_eff3_1W.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/18-01-10-eff3.pth')['weight'])\n",
        "Efficient_acc_eff3_1W.to(device)\n",
        "Efficient_acc_eff3_1W.eval()\n",
        "\n",
        "Efficient_acc_eff2_1W = get_model('eff2')\n",
        "Efficient_acc_eff2_1W.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/12-52-55-eff4.pth')['weight'])\n",
        "Efficient_acc_eff2_1W.to(device)\n",
        "Efficient_acc_eff2_1W.eval()\n",
        "\n",
        "\n",
        "Efficient_acc_eff1_1W = get_model('eff1')\n",
        "Efficient_acc_eff1_1W.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/12-59-11-eff3.pth')['weight'])\n",
        "Efficient_acc_eff1_1W.to(device)\n",
        "Efficient_acc_eff1_1W.eval()\n",
        "\n",
        "Efficient_acc_eff0_1W = get_model('eff0')\n",
        "Efficient_acc_eff0_1W.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/12-59-11-eff3.pth')['weight'])\n",
        "Efficient_acc_eff0_1W.to(device)\n",
        "Efficient_acc_eff0_1W.eval()\n",
        "\n",
        "Vit_base_acc_1W=get_model('vit_base')\n",
        "Vit_base_acc_1W.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/19-33-48-vit_base.pth')['weight'])\n",
        "Vit_base_acc_1W.to(device)\n",
        "Vit_base_acc_1W.eval()\n",
        "\n",
        "ResNet_50_acc_1W = get_model('ResNet_50')\n",
        "ResNet_50_acc_1W.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/18-04-45-ResNet_50.pth')['weight'])\n",
        "ResNet_50_acc_1W.to(device)\n",
        "ResNet_50_acc_1W.eval()\n",
        "\n",
        "ResNet_34_acc_1W = get_model('ResNet_34')\n",
        "ResNet_34_acc_1W.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/18-29-04-ResNet_34.pth')['weight'])\n",
        "ResNet_34_acc_1W.to(device)\n",
        "ResNet_34_acc_1W.eval()\n",
        "\n",
        "# Optimizer: AdamW learning scheduler:  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\n",
        "\n",
        "Efficient_acc_eff4_3W = get_model('eff4')\n",
        "Efficient_acc_eff4_3W.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/18-29-51-eff4.pth')['weight'])\n",
        "Efficient_acc_eff4_3W.to(device)\n",
        "Efficient_acc_eff4_3W.eval()\n",
        "\n",
        "Efficient_acc_eff3_3W = get_model('eff3')\n",
        "Efficient_acc_eff3_3W.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/18-38-14-eff3.pth')['weight'])\n",
        "Efficient_acc_eff3_3W.to(device)\n",
        "Efficient_acc_eff3_3W.eval()\n",
        "\n",
        "Efficient_acc_eff2_3W = get_model('eff2')\n",
        "Efficient_acc_eff2_3W.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/12-52-55-eff4.pth')['weight'])\n",
        "Efficient_acc_eff2_3W.to(device)\n",
        "Efficient_acc_eff2_3W.eval()\n",
        "\n",
        "Efficient_acc_eff1_3W = get_model('eff1')\n",
        "Efficient_acc_eff1_3W.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/12-59-11-eff3.pth')['weight'])\n",
        "Efficient_acc_eff1_3W.to(device)\n",
        "Efficient_acc_eff1_3W.eval()\n",
        "\n",
        "Efficient_acc_eff0_3W = get_model('eff0')\n",
        "Efficient_acc_eff0_3W.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/12-59-11-eff3.pth')['weight'])\n",
        "Efficient_acc_eff0_3W.to(device)\n",
        "Efficient_acc_eff0_3W.eval()\n",
        "\n",
        "Vit_base_acc_3W=get_model('vit_base')\n",
        "Vit_base_acc_3W.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/20-03-39-vit_base.pth')['weight'])\n",
        "Vit_base_acc_3W.to(device)\n",
        "Vit_base_acc_3W.eval()\n",
        "\n",
        "ResNet_50_acc_3W = get_model('ResNet_50')\n",
        "ResNet_50_acc_3W.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/18-42-40-ResNet_50.pth')['weight'])\n",
        "ResNet_50_acc_3W.to(device)\n",
        "ResNet_50_acc_3W.eval()\n",
        "\n",
        "ResNet_34_acc_3W = get_model('ResNet_34')\n",
        "ResNet_34_acc_3W.load_state_dict(torch.load('/content/log/best_weight/2021-09-26/18-53-12-ResNet_34.pth')['weight'])\n",
        "ResNet_34_acc_3W.to(device)\n",
        "ResNet_34_acc_3W.eval()\n"
      ],
      "id": "ykHGMiOatS6C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH3SIJlq3AcE"
      },
      "source": [
        "batch_size=1\n",
        "\n",
        "def Ensemble():\n",
        "  for idx,data in enumerate(test_loader):\n",
        "    images, labels = data\n",
        "    images, labels = images.cuda(), labels.cuda()\n",
        "    \n",
        "\n",
        "    outputs = Efficient_acc_eff4_1A(images)\n",
        "    outputs += Efficient_acc_eff3_1A(images)\n",
        "    outputs += Efficient_acc_eff2_1A(images)\n",
        "    outputs += Efficient_acc_eff1_1A(images)\n",
        "    outputs += Efficient_acc_eff0_1A(images)\n",
        "    outputs += ResNet_50_acc_1A(images)\n",
        "    outputs += ResNet_34_acc_1A(images)\n",
        "\n",
        "    outputs += Efficient_acc_eff4_2A(images)\n",
        "    outputs += Efficient_acc_eff3_2A(images)\n",
        "    outputs += Efficient_acc_eff2_2A(images)\n",
        "    outputs += Efficient_acc_eff1_2A(images)\n",
        "    outputs += Efficient_acc_eff0_2A(images)\n",
        "    outputs += ResNet_50_acc_2A(images)\n",
        "    outputs += ResNet_34_acc_2A(images)\n",
        "\n",
        "    outputs += Efficient_acc_eff4_3A(images)\n",
        "    outputs += Efficient_acc_eff3_3A(images)\n",
        "    outputs += Efficient_acc_eff2_3A(images)\n",
        "    outputs += Efficient_acc_eff1_3A(images)\n",
        "    outputs += Efficient_acc_eff0_3A(images)\n",
        "    outputs += ResNet_50_acc_3A(images)\n",
        "    outputs += ResNet_34_acc_3A(images)\n",
        "\n",
        "    outputs += Efficient_acc_eff4_1R(images)\n",
        "    outputs += Efficient_acc_eff3_1R(images)\n",
        "    outputs += Efficient_acc_eff2_1R(images)\n",
        "    outputs += Efficient_acc_eff1_1R(images)\n",
        "    outputs += Efficient_acc_eff0_1R(images)\n",
        "    outputs += ResNet_50_acc_1R(images)\n",
        "    outputs += ResNet_34_acc_1R(images)\n",
        "\n",
        "    outputs += Efficient_acc_eff4_2R(images)\n",
        "    outputs += Efficient_acc_eff3_2R(images)\n",
        "    outputs += Efficient_acc_eff2_2R(images)\n",
        "    outputs += Efficient_acc_eff1_2R(images)\n",
        "    outputs += Efficient_acc_eff0_2R(images)\n",
        "    outputs += ResNet_50_acc_2R(images)\n",
        "    outputs += ResNet_34_acc_2R(images)\n",
        "\n",
        "    outputs += Efficient_acc_eff4_3R(images)\n",
        "    outputs += Efficient_acc_eff3_3R(images)\n",
        "    outputs += Efficient_acc_eff2_3R(images)\n",
        "    outputs += Efficient_acc_eff1_3R(images)\n",
        "    outputs += Efficient_acc_eff0_3R(images)\n",
        "    outputs += ResNet_50_acc_3R(images)\n",
        "    outputs += ResNet_34_acc_3R(images)\n",
        "\n",
        "    outputs += Efficient_acc_eff4_1W(images)\n",
        "    outputs += Efficient_acc_eff3_1W(images)\n",
        "    outputs += Efficient_acc_eff2_1W(images)\n",
        "    outputs += Efficient_acc_eff1_1W(images)\n",
        "    outputs += Efficient_acc_eff0_1W(images)\n",
        "    outputs += ResNet_50_acc_1W(images)\n",
        "    outputs += ResNet_34_acc_1W(images)\n",
        "\n",
        "    outputs += Efficient_acc_eff4_2W(images)\n",
        "    outputs += Efficient_acc_eff3_2W(images)\n",
        "    outputs += Efficient_acc_eff2_2W(images)\n",
        "    outputs += Efficient_acc_eff1_2W(images)\n",
        "    outputs += Efficient_acc_eff0_2W(images)\n",
        "    outputs += ResNet_50_acc_2W(images)\n",
        "    outputs += ResNet_34_acc_2W(images)\n",
        "\n",
        "    outputs += Efficient_acc_eff4_3W(images)\n",
        "    outputs += Efficient_acc_eff3_3W(images)\n",
        "    outputs += Efficient_acc_eff2_3W(images)\n",
        "    outputs += Efficient_acc_eff1_3W(images)\n",
        "    outputs += Efficient_acc_eff0_3W(images)\n",
        "    outputs += ResNet_50_acc_3W(images)\n",
        "    outputs += ResNet_34_acc_3W(images)\n",
        "\n",
        "    _, predictions = torch.max(outputs, 1)\n",
        "    predictions= torch.reshape(predictions,(-1,1))\n",
        "    batch_index=batch_size*idx\n",
        "    probs_array[batch_index:batch_index+batch_size]= predictions.cpu()\n",
        "  return probs_array\n",
        "\n",
        "sample_submission = pd.read_csv(\"https://raw.githubusercontent.com/cjfghk5697/AjouDeeplearning_Compete/main/sample_submission.csv\")\n",
        "probs_array = np.zeros([sample_submission.shape[0],\n",
        "                              sample_submission.shape[1] -1])\n",
        "probs_array=Ensemble()\n",
        "sample_submission.iloc[:,1:] = probs_array\n",
        "sample_submission=sample_submission.astype(int)\n",
        "sample_submission.to_csv('efficientNet_Ensemble.csv',index = False) # 경로 수정 필요 "
      ],
      "id": "oH3SIJlq3AcE",
      "execution_count": null,
      "outputs": []
    }
  ]
}