{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cjfghk5697/AjouDeeplearning_Compete/blob/main/%EC%98%A4%EB%A5%98%20%EC%88%98%EC%A0%95.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwCqIvW86tST"
      },
      "source": [
        "# 1. 이진화 코드 추가\n",
        " 이진화란 이미지를 흰색과 검은색으로 바꾼다고 생각하면 된다.\n",
        "속도가 향상되고 정확도도 미약하지만 향상된것을 볼수 있다.\n",
        "\n",
        " [Dacon 적용 예시](https://dacon.io/competitions/official/235697/codeshare/2436?page=2&dtype=recent)\n",
        "\n",
        "\n",
        " [이진화 설명](https://gmnam.tistory.com/263)\n",
        "\n",
        " <hr>\n",
        "\n",
        " # 2. train, valid set 나누기\n",
        "```\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split(trainset, [train_size, valid_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, num_workers=8)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=64, num_workers=8)\n",
        "```\n",
        "처음에는 나누어지지 않아있는 코드였단 걸 발견하고 random_split 함수를 이용해 나눔. 그리고 학습 부분에서 valid 데이터를 enumerate하는 구문추가해서 valid 확인.\n",
        "\n",
        "<hr>\n",
        "\n",
        "# 3. Multi Sample Dropout 구현\n",
        "\n",
        "[자료](https://velog.io/@sujeongim/%EC%83%88%EB%A1%9C-%EB%B0%B0%EC%9A%B4-%EC%BD%94%EB%93%9C-%EC%A1%B0%EA%B0%81-Multi-Sample-Dropout)\n",
        "단순 dropout 기법을 안썼다. 근데 큰효과가 있는지는 잘 모르지만 overfitting의 위협도 없앨겸 선택했다.\n",
        "\n",
        "<hr>\n",
        "\n",
        "# 4. Scheduler 추가\n",
        "일단 간단하게 StepLR scheduler을 추가했다.\n",
        "\n",
        "<hr>\n",
        "\n",
        "# 5. Drop out 실수\n",
        "전 Drop out에서는 sigmoid가 있다. 근데 필자가 Multi-Label Loss Function을 이용함으로써 활성화 함수를 더 추가할 필요가 없었다. 그러나 sigmoid를 또 넣음으로써 sigmoid 계층 2개를 들어가는 거다. 결국 loss 값이 적어져서 학습 진행이 안된거다. 학습이 잘 안되면 활성화 함수에서 loss 값이 얼마 나오는지 잘 확인해야겠다..\n",
        "\n",
        "https://cvml.tistory.com/26 \n",
        "Multi-Label Loss Function에 대해 잘 설명 되어있다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwV9U9iOB8M4",
        "outputId": "1757d4d3-981a-4935-e5b1-c1f1908a99c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "44kva0aP7lPz"
      },
      "outputs": [],
      "source": [
        "#%cd /content/drive/MyDrive/Test image2\n",
        "\n",
        "#!unzip -qq \"/content/drive/MyDrive/test_dirty_mnist_2nd.zip\"\n",
        "\n",
        "#%cd /content/drive/MyDrive/Train image2\n",
        "#!unzip -qq \"/content/drive/MyDrive/dirty_mnist_2nd.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gF4174-lYKPy",
        "outputId": "665575bb-2637-4ad7-c590-d1df4a2e1ab2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.7/dist-packages (1.6.5)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement cv2 (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for cv2\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torchinfo\n",
        "!pip3 install cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RTMT6OMkBtpa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Tuple, Sequence, Callable\n",
        "import csv\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch import nn, Tensor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchinfo import summary\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet50\n",
        "#이진화 추가\n",
        "import cv2\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQMtnKh9Btpi"
      },
      "source": [
        "## 1. 커스텀 데이터셋 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NjkhhJXmBtpl"
      },
      "outputs": [],
      "source": [
        "class MnistDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dir: os.PathLike,\n",
        "        image_ids: os.PathLike,\n",
        "        transforms: Sequence[Callable]\n",
        "    ) -> None:\n",
        "        self.dir = dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        self.labels = {}\n",
        "        with open(image_ids, 'r') as f:\n",
        "            reader = csv.reader(f)\n",
        "            next(reader)\n",
        "            for row in reader:\n",
        "                self.labels[int(row[0])] = list(map(int, row[1:]))\n",
        "\n",
        "        self.image_ids = list(self.labels.keys())\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[Tensor]:\n",
        "        image_id = self.image_ids[index]\n",
        "        image = Image.open(\n",
        "            os.path.join(\n",
        "                self.dir, f'{str(image_id).zfill(5)}.png')).convert('RGB')\n",
        "        target = np.array(self.labels.get(image_id)).astype(np.float32)\n",
        "\n",
        "\n",
        "        image = np.array(image)\n",
        "#이진화 완료 cv2 툴을 써서 threshold로 254 값으로 변환. cv2는 넘파이만 사용 가능해서 넘파이로 변환후 fromarry로 pil로 바꾼다.\n",
        "#        image_th = cv2.threshold(image,254,255,0)[1]\n",
        "        image_th = Image.fromarray(image) # NumPy array to PIL image\n",
        "        if self.transforms is not None:\n",
        "            image = self.transforms(image_th)\n",
        "\n",
        "        return image, target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoSfoQnxBtpo"
      },
      "source": [
        "## 2. 이미지 어그멘테이션"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GJ0iMdDKBtpp"
      },
      "outputs": [],
      "source": [
        "transforms_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        [0.485, 0.456, 0.406],\n",
        "        [0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "transforms_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        [0.485, 0.456, 0.406],\n",
        "        [0.229, 0.224, 0.225]\n",
        "    )\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTvYYjNnBtpr",
        "outputId": "8639f3f9-3205-4cec-e844-210f1348a4fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "trainset = MnistDataset('/content/drive/MyDrive/Train image2', '/content/drive/MyDrive/dirty_mnist_2nd_answer.csv', transforms_train)\n",
        "testset = MnistDataset('/content/drive/MyDrive/Test image2', '/content/drive/MyDrive/sample_submission.csv', transforms_test)\n",
        "\n",
        "train_size = int(0.8 * len(trainset))\n",
        "valid_size = len(trainset) - train_size\n",
        "\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split(trainset, [train_size, valid_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, num_workers=8)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=16, num_workers=8)\n",
        "\n",
        "test_loader = DataLoader(testset, batch_size=32, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJV1syjOBtps"
      },
      "source": [
        "## 3. ResNet50 모형"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kziKrm1tBtpt",
        "outputId": "2e6e63d2-332f-4012-b252-c9f1bb7f480f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===============================================================================================\n",
            "Layer (type:depth-idx)                        Output Shape              Param #\n",
            "===============================================================================================\n",
            "MnistModel                                    --                        --\n",
            "├─ModuleList: 1-1                             --                        --\n",
            "├─ResNet: 1-2                                 [1, 1000]                 --\n",
            "│    └─Conv2d: 2-1                            [1, 64, 128, 128]         9,408\n",
            "│    └─BatchNorm2d: 2-2                       [1, 64, 128, 128]         128\n",
            "│    └─ReLU: 2-3                              [1, 64, 128, 128]         --\n",
            "│    └─MaxPool2d: 2-4                         [1, 64, 64, 64]           --\n",
            "│    └─Sequential: 2-5                        [1, 256, 64, 64]          --\n",
            "│    │    └─Bottleneck: 3-1                   [1, 256, 64, 64]          75,008\n",
            "│    │    └─Bottleneck: 3-2                   [1, 256, 64, 64]          70,400\n",
            "│    │    └─Bottleneck: 3-3                   [1, 256, 64, 64]          70,400\n",
            "│    └─Sequential: 2-6                        [1, 512, 32, 32]          --\n",
            "│    │    └─Bottleneck: 3-4                   [1, 512, 32, 32]          379,392\n",
            "│    │    └─Bottleneck: 3-5                   [1, 512, 32, 32]          280,064\n",
            "│    │    └─Bottleneck: 3-6                   [1, 512, 32, 32]          280,064\n",
            "│    │    └─Bottleneck: 3-7                   [1, 512, 32, 32]          280,064\n",
            "│    └─Sequential: 2-7                        [1, 1024, 16, 16]         --\n",
            "│    │    └─Bottleneck: 3-8                   [1, 1024, 16, 16]         1,512,448\n",
            "│    │    └─Bottleneck: 3-9                   [1, 1024, 16, 16]         1,117,184\n",
            "│    │    └─Bottleneck: 3-10                  [1, 1024, 16, 16]         1,117,184\n",
            "│    │    └─Bottleneck: 3-11                  [1, 1024, 16, 16]         1,117,184\n",
            "│    │    └─Bottleneck: 3-12                  [1, 1024, 16, 16]         1,117,184\n",
            "│    │    └─Bottleneck: 3-13                  [1, 1024, 16, 16]         1,117,184\n",
            "├─ModuleList: 1-3                             --                        --\n",
            "│    └─Dropout: 2-8                           --                        --\n",
            "│    └─Dropout: 2-9                           --                        --\n",
            "│    └─Dropout: 2-10                          --                        --\n",
            "│    └─Dropout: 2-11                          --                        --\n",
            "│    └─Dropout: 2-12                          --                        --\n",
            "├─ResNet: 1                                   --                        --\n",
            "│    └─Sequential: 2-13                       [1, 2048, 8, 8]           --\n",
            "│    │    └─Bottleneck: 3-14                  [1, 2048, 8, 8]           6,039,552\n",
            "│    │    └─Bottleneck: 3-15                  [1, 2048, 8, 8]           4,462,592\n",
            "│    │    └─Bottleneck: 3-16                  [1, 2048, 8, 8]           4,462,592\n",
            "│    └─AdaptiveAvgPool2d: 2-14                [1, 2048, 1, 1]           --\n",
            "│    └─Linear: 2-15                           [1, 1000]                 2,049,000\n",
            "├─Linear: 1-4                                 [1, 26]                   26,026\n",
            "===============================================================================================\n",
            "Total params: 25,583,058\n",
            "Trainable params: 25,583,058\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 5.34\n",
            "===============================================================================================\n",
            "Input size (MB): 0.79\n",
            "Forward/backward pass size (MB): 232.27\n",
            "Params size (MB): 102.33\n",
            "Estimated Total Size (MB): 335.39\n",
            "===============================================================================================\n"
          ]
        }
      ],
      "source": [
        "class MnistModel(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.resnet = resnet50(pretrained=True)\n",
        "        self.classifier = nn.Linear(1000, 26)\n",
        "         # dropout samples 생성 \n",
        "        self.dropouts = nn.ModuleList([\n",
        "            nn.Dropout(0.5) for _ in range(5)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet(x)\n",
        "\n",
        "#multi sample Dropout 구현. \n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = MnistModel().to(device)\n",
        "print(summary(model, input_size=(1, 3, 256, 256), verbose=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI_79bn8Btpw"
      },
      "source": [
        "## 4. 학습하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZ3j-bG4Btpy",
        "outputId": "89dcaeb5-0767-4aec-b575-234935ce86fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: \"train loss:\"0.72246, \"train acc: \"0.52404\n",
            "0: \"train loss:\"0.70455, \"train acc: \"0.53005\n",
            "0: \"train loss:\"0.69920, \"train acc: \"0.54447\n",
            "0: \"train loss:\"0.70845, \"train acc: \"0.53005\n",
            "0: \"train loss:\"0.69716, \"train acc: \"0.55529\n",
            "0: \"train loss:\"0.69783, \"train acc: \"0.54087\n",
            "0: \"train loss:\"0.69726, \"train acc: \"0.54327\n",
            "0: \"train loss:\"0.69407, \"train acc: \"0.53245\n",
            "0: \"train loss:\"0.69972, \"train acc: \"0.54087\n",
            "0: \"train loss:\"0.70033, \"train acc: \"0.53486\n",
            "0: \"train loss:\"0.68978, \"train acc: \"0.55649\n",
            "0: \"train loss:\"0.69635, \"train acc: \"0.54567\n",
            "0: \"train loss:\"0.70149, \"train acc: \"0.52644\n",
            "0: \"train loss:\"0.68717, \"train acc: \"0.53245\n",
            "0: \"train loss:\"0.68504, \"train acc: \"0.55769\n",
            "0: \"train loss:\"0.69381, \"train acc: \"0.53005\n",
            "0: \"train loss:\"0.68887, \"train acc: \"0.54327\n",
            "0: \"train loss:\"0.69073, \"train acc: \"0.52885\n",
            "0: \"train loss:\"0.69346, \"train acc: \"0.54567\n",
            "0: \"train loss:\"0.69617, \"train acc: \"0.53005\n",
            "0: \"train loss:\"0.69142, \"train acc: \"0.52644\n",
            "0: \"train loss:\"0.69283, \"train acc: \"0.52644\n",
            "0: \"train loss:\"0.69018, \"train acc: \"0.54688\n",
            "0: \"train loss:\"0.68958, \"train acc: \"0.54447\n",
            "0: \"train loss:\"0.69475, \"train acc: \"0.53486\n",
            "0: \"train loss:\"0.69514, \"train acc: \"0.52524\n",
            "0: \"train loss:\"0.68814, \"train acc: \"0.54327\n",
            "0: \"train loss:\"0.69170, \"train acc: \"0.53846\n",
            "0: \"train loss:\"0.69140, \"train acc: \"0.53846\n",
            "0: \"train loss:\"0.69242, \"train acc: \"0.52644\n",
            "0: \"train loss:\"0.68964, \"train acc: \"0.53245\n",
            "0: \"train loss:\"0.69614, \"train acc: \"0.53125\n",
            "0: \"train loss:\"0.69083, \"train acc: \"0.53486\n",
            "0: \"train loss:\"0.68829, \"train acc: \"0.54567\n",
            "0: \"train loss:\"0.68985, \"train acc: \"0.53005\n",
            "0: \"train loss:\"0.68626, \"train acc: \"0.54567\n",
            "0: \"train loss:\"0.68995, \"train acc: \"0.53486\n",
            "0: \"train loss:\"0.68807, \"train acc: \"0.52764\n",
            "0: \"train loss:\"0.68812, \"train acc: \"0.53966\n",
            "0: \"train loss:\"0.68996, \"train acc: \"0.53726\n",
            "0: \"train loss:\"0.69058, \"train acc: \"0.53005\n",
            "0: \"train loss:\"0.69244, \"train acc: \"0.54808\n",
            "0: \"train loss:\"0.68705, \"train acc: \"0.54087\n",
            "0: \"train loss:\"0.69093, \"train acc: \"0.53966\n",
            "0: \"train loss:\"0.69033, \"train acc: \"0.54207\n",
            "0: \"train loss:\"0.69197, \"train acc: \"0.53245\n",
            "0: \"train loss:\"0.69310, \"train acc: \"0.53125\n",
            "0: \"train loss:\"0.69009, \"train acc: \"0.53125\n",
            "0: \"train loss:\"0.68989, \"train acc: \"0.53125\n",
            "0: \"train loss:\"0.69231, \"train acc: \"0.52284\n",
            "0: \"train loss:\"0.68814, \"train acc: \"0.52885\n",
            "0: \"train loss:\"0.69036, \"train acc: \"0.54928\n",
            "0: \"train loss:\"0.69250, \"train acc: \"0.52885\n",
            "0: \"train loss:\"0.68513, \"train acc: \"0.55649\n",
            "0: \"train loss:\"0.69300, \"train acc: \"0.53606\n",
            "0: \"train loss:\"0.68957, \"train acc: \"0.52404\n",
            "0: \"train loss:\"0.68934, \"train acc: \"0.53726\n",
            "0: \"train loss:\"0.68632, \"train acc: \"0.56731\n",
            "0: \"train loss:\"0.68917, \"train acc: \"0.53245\n",
            "0: \"train loss:\"0.68830, \"train acc: \"0.54567\n",
            "0: \"train loss:\"0.68895, \"train acc: \"0.53125\n",
            "0: \"train loss:\"0.69566, \"train acc: \"0.53005\n",
            "0: \"train loss:\"0.68936, \"train acc: \"0.52043\n",
            "0: \"train loss:\"0.68491, \"train acc: \"0.55889\n",
            "0: \"train loss:\"0.68945, \"train acc: \"0.54327\n",
            "0: \"train loss:\"0.68818, \"train acc: \"0.53966\n",
            "0: \"train loss:\"0.68850, \"train acc: \"0.54327\n",
            "0: \"train loss:\"0.68642, \"train acc: \"0.53005\n",
            "0: \"train loss:\"0.68847, \"train acc: \"0.55529\n",
            "0: \"train loss:\"0.68885, \"train acc: \"0.54087\n",
            "0: \"train loss:\"0.68839, \"train acc: \"0.53966\n",
            "0: \"train loss:\"0.68714, \"train acc: \"0.53245\n",
            "0: \"train loss:\"0.68857, \"train acc: \"0.53606\n",
            "0: \"train loss:\"0.69303, \"train acc: \"0.53846\n",
            "0: \"train loss:\"0.68786, \"train acc: \"0.53365\n",
            "0: \"train loss:\"0.68947, \"train acc: \"0.53966\n",
            "0: \"train loss:\"0.69068, \"train acc: \"0.53245\n",
            "0: \"train loss:\"0.68679, \"train acc: \"0.53846\n",
            "0: \"train loss:\"0.68689, \"train acc: \"0.53125\n",
            "0: \"train loss:\"0.68721, \"train acc: \"0.53125\n",
            "0: \"train loss:\"0.68495, \"train acc: \"0.54928\n",
            "0: \"train loss:\"0.68455, \"train acc: \"0.53726\n",
            "0: \"train loss:\"0.68666, \"train acc: \"0.54087\n",
            "0: \"train loss:\"0.69012, \"train acc: \"0.53365\n",
            "0: \"train loss:\"0.68600, \"train acc: \"0.53486\n",
            "0: \"train loss:\"0.68775, \"train acc: \"0.54207\n",
            "0: \"train loss:\"0.68980, \"train acc: \"0.54327\n",
            "0: \"train loss:\"0.69339, \"train acc: \"0.53005\n",
            "0: \"train loss:\"0.68487, \"train acc: \"0.54447\n",
            "0: \"train loss:\"0.68724, \"train acc: \"0.55409\n",
            "0: \"train loss:\"0.68690, \"train acc: \"0.53966\n",
            "0: \"train loss:\"0.68700, \"train acc: \"0.54087\n",
            "0: \"train loss:\"0.68914, \"train acc: \"0.53005\n",
            "0: \"train loss:\"0.68289, \"train acc: \"0.55288\n",
            "0: \"train loss:\"0.68816, \"train acc: \"0.53606\n",
            "0: \"train loss:\"0.68741, \"train acc: \"0.54447\n",
            "0: \"train loss:\"0.68663, \"train acc: \"0.54087\n",
            "0: \"train loss:\"0.68534, \"train acc: \"0.55288\n",
            "0: \"train loss:\"0.68035, \"train acc: \"0.54808\n",
            "0: \"train loss:\"0.68051, \"train acc: \"0.53726\n",
            "0: \"train loss:\"0.68655, \"train acc: \"0.53726\n",
            "0: \"train loss:\"0.68697, \"train acc: \"0.54928\n",
            "0: \"train loss:\"0.68754, \"train acc: \"0.54207\n",
            "0: \"train loss:\"0.68018, \"train acc: \"0.53966\n",
            "0: \"train loss:\"0.68650, \"train acc: \"0.53846\n",
            "0: \"train loss:\"0.68809, \"train acc: \"0.53606\n",
            "0: \"train loss:\"0.68793, \"train acc: \"0.53846\n",
            "0: \"train loss:\"0.68597, \"train acc: \"0.55168\n",
            "0: \"train loss:\"0.68171, \"train acc: \"0.54207\n",
            "0: \"train loss:\"0.68543, \"train acc: \"0.54207\n",
            "0: \"train loss:\"0.68197, \"train acc: \"0.54087\n",
            "0: \"train loss:\"0.68285, \"train acc: \"0.54567\n",
            "0: \"train loss:\"0.69079, \"train acc: \"0.52404\n",
            "0: \"train loss:\"0.68843, \"train acc: \"0.53125\n",
            "0: \"train loss:\"0.68487, \"train acc: \"0.54327\n",
            "0: \"train loss:\"0.68680, \"train acc: \"0.53486\n",
            "0: \"train loss:\"0.68372, \"train acc: \"0.55889\n",
            "0: \"train loss:\"0.68714, \"train acc: \"0.54327\n",
            "0: \"train loss:\"0.68751, \"train acc: \"0.53726\n",
            "0: \"train loss:\"0.68989, \"train acc: \"0.52404\n",
            "0: \"train loss:\"0.68883, \"train acc: \"0.53365\n",
            "0: \"train loss:\"0.68040, \"train acc: \"0.54808\n",
            "0: \"train loss:\"0.68254, \"train acc: \"0.54688\n",
            "0: \"train loss:\"0.68034, \"train acc: \"0.55048\n",
            "0: \"train loss:\"0.68685, \"train acc: \"0.52885\n",
            "0: \"val loss:\"0.67955, \"val acc: \"0.53606\n",
            "0: \"val loss:\"0.67227, \"val acc: \"0.58413\n",
            "0: \"val loss:\"0.67392, \"val acc: \"0.55769\n",
            "0: \"val loss:\"0.68374, \"val acc: \"0.53846\n",
            "0: \"val loss:\"0.67796, \"val acc: \"0.54567\n",
            "0: \"val loss:\"0.68949, \"val acc: \"0.54567\n",
            "0: \"val loss:\"0.68520, \"val acc: \"0.53846\n",
            "0: \"val loss:\"0.67737, \"val acc: \"0.54808\n",
            "0: \"val loss:\"0.68621, \"val acc: \"0.54087\n",
            "0: \"val loss:\"0.67476, \"val acc: \"0.54808\n",
            "0: \"val loss:\"0.69055, \"val acc: \"0.51202\n",
            "0: \"val loss:\"0.68701, \"val acc: \"0.54567\n",
            "0: \"val loss:\"0.68842, \"val acc: \"0.54087\n",
            "0: \"val loss:\"0.68892, \"val acc: \"0.55288\n",
            "0: \"val loss:\"0.69322, \"val acc: \"0.54087\n",
            "0: \"val loss:\"0.68872, \"val acc: \"0.56250\n",
            "0: \"val loss:\"0.69255, \"val acc: \"0.52644\n",
            "0: \"val loss:\"0.68780, \"val acc: \"0.55529\n",
            "0: \"val loss:\"0.67341, \"val acc: \"0.55288\n",
            "0: \"val loss:\"0.68478, \"val acc: \"0.55769\n",
            "0: \"val loss:\"0.68414, \"val acc: \"0.54808\n",
            "0: \"val loss:\"0.68641, \"val acc: \"0.56010\n",
            "0: \"val loss:\"0.68087, \"val acc: \"0.56250\n",
            "0: \"val loss:\"0.68805, \"val acc: \"0.54087\n",
            "0: \"val loss:\"0.68117, \"val acc: \"0.54567\n",
            "0: \"val loss:\"0.68194, \"val acc: \"0.56250\n",
            "0: \"val loss:\"0.68412, \"val acc: \"0.54327\n",
            "0: \"val loss:\"0.67762, \"val acc: \"0.54567\n",
            "0: \"val loss:\"0.68214, \"val acc: \"0.56010\n",
            "0: \"val loss:\"0.68711, \"val acc: \"0.53606\n",
            "0: \"val loss:\"0.68217, \"val acc: \"0.56490\n",
            "0: \"val loss:\"0.68002, \"val acc: \"0.54567\n",
            "0: \"val loss:\"0.69024, \"val acc: \"0.51683\n",
            "0: \"val loss:\"0.68581, \"val acc: \"0.55048\n",
            "0: \"val loss:\"0.68289, \"val acc: \"0.52885\n",
            "0: \"val loss:\"0.68058, \"val acc: \"0.54327\n",
            "0: \"val loss:\"0.68037, \"val acc: \"0.54327\n",
            "0: \"val loss:\"0.67936, \"val acc: \"0.54087\n",
            "0: \"val loss:\"0.68796, \"val acc: \"0.55288\n",
            "0: \"val loss:\"0.67783, \"val acc: \"0.57212\n",
            "0: \"val loss:\"0.68802, \"val acc: \"0.54087\n",
            "0: \"val loss:\"0.68454, \"val acc: \"0.55048\n",
            "0: \"val loss:\"0.68751, \"val acc: \"0.53606\n",
            "0: \"val loss:\"0.68584, \"val acc: \"0.54087\n",
            "0: \"val loss:\"0.67133, \"val acc: \"0.55769\n",
            "0: \"val loss:\"0.68110, \"val acc: \"0.53846\n",
            "0: \"val loss:\"0.66977, \"val acc: \"0.55048\n",
            "0: \"val loss:\"0.68051, \"val acc: \"0.56010\n",
            "0: \"val loss:\"0.68585, \"val acc: \"0.55529\n",
            "0: \"val loss:\"0.68290, \"val acc: \"0.55048\n",
            "0: \"val loss:\"0.68856, \"val acc: \"0.52644\n",
            "0: \"val loss:\"0.68809, \"val acc: \"0.53125\n",
            "0: \"val loss:\"0.67321, \"val acc: \"0.58413\n",
            "0: \"val loss:\"0.68085, \"val acc: \"0.56731\n",
            "0: \"val loss:\"0.69085, \"val acc: \"0.53606\n",
            "0: \"val loss:\"0.69066, \"val acc: \"0.53125\n",
            "0: \"val loss:\"0.67841, \"val acc: \"0.54087\n",
            "0: \"val loss:\"0.69345, \"val acc: \"0.52404\n",
            "0: \"val loss:\"0.68509, \"val acc: \"0.55529\n",
            "0: \"val loss:\"0.68055, \"val acc: \"0.56010\n",
            "0: \"val loss:\"0.69016, \"val acc: \"0.50721\n",
            "0: \"val loss:\"0.67921, \"val acc: \"0.53846\n",
            "1: \"train loss:\"0.69013, \"train acc: \"0.52644\n",
            "1: \"train loss:\"0.68009, \"train acc: \"0.53365\n",
            "1: \"train loss:\"0.68093, \"train acc: \"0.54447\n",
            "1: \"train loss:\"0.68306, \"train acc: \"0.53245\n",
            "1: \"train loss:\"0.67639, \"train acc: \"0.57091\n",
            "1: \"train loss:\"0.68722, \"train acc: \"0.54567\n",
            "1: \"train loss:\"0.68747, \"train acc: \"0.54447\n",
            "1: \"train loss:\"0.68670, \"train acc: \"0.52764\n",
            "1: \"train loss:\"0.68066, \"train acc: \"0.54688\n",
            "1: \"train loss:\"0.68519, \"train acc: \"0.54928\n",
            "1: \"train loss:\"0.67620, \"train acc: \"0.56010\n",
            "1: \"train loss:\"0.68294, \"train acc: \"0.55048\n",
            "1: \"train loss:\"0.69037, \"train acc: \"0.53005\n",
            "1: \"train loss:\"0.68035, \"train acc: \"0.54087\n",
            "1: \"train loss:\"0.67402, \"train acc: \"0.56250\n",
            "1: \"train loss:\"0.68157, \"train acc: \"0.53846\n",
            "1: \"train loss:\"0.68426, \"train acc: \"0.54567\n",
            "1: \"train loss:\"0.68047, \"train acc: \"0.53486\n",
            "1: \"train loss:\"0.67868, \"train acc: \"0.55529\n",
            "1: \"train loss:\"0.67306, \"train acc: \"0.54327\n",
            "1: \"train loss:\"0.67770, \"train acc: \"0.53125\n",
            "1: \"train loss:\"0.68335, \"train acc: \"0.53726\n",
            "1: \"train loss:\"0.67302, \"train acc: \"0.55168\n",
            "1: \"train loss:\"0.67825, \"train acc: \"0.55288\n",
            "1: \"train loss:\"0.68268, \"train acc: \"0.53486\n",
            "1: \"train loss:\"0.68415, \"train acc: \"0.53606\n",
            "1: \"train loss:\"0.67973, \"train acc: \"0.54447\n",
            "1: \"train loss:\"0.68478, \"train acc: \"0.55168\n",
            "1: \"train loss:\"0.67956, \"train acc: \"0.54567\n",
            "1: \"train loss:\"0.68281, \"train acc: \"0.53365\n",
            "1: \"train loss:\"0.68245, \"train acc: \"0.53966\n",
            "1: \"train loss:\"0.68670, \"train acc: \"0.53486\n",
            "1: \"train loss:\"0.67714, \"train acc: \"0.54327\n",
            "1: \"train loss:\"0.68351, \"train acc: \"0.55409\n",
            "1: \"train loss:\"0.68189, \"train acc: \"0.53365\n",
            "1: \"train loss:\"0.67949, \"train acc: \"0.55409\n",
            "1: \"train loss:\"0.67718, \"train acc: \"0.55048\n",
            "1: \"train loss:\"0.67532, \"train acc: \"0.53726\n",
            "1: \"train loss:\"0.66993, \"train acc: \"0.55409\n",
            "1: \"train loss:\"0.68183, \"train acc: \"0.54087\n",
            "1: \"train loss:\"0.67665, \"train acc: \"0.55409\n",
            "1: \"train loss:\"0.69016, \"train acc: \"0.55168\n",
            "1: \"train loss:\"0.67969, \"train acc: \"0.54567\n",
            "1: \"train loss:\"0.67283, \"train acc: \"0.55288\n",
            "1: \"train loss:\"0.67611, \"train acc: \"0.55529\n",
            "1: \"train loss:\"0.67691, \"train acc: \"0.54327\n",
            "1: \"train loss:\"0.67350, \"train acc: \"0.54808\n",
            "1: \"train loss:\"0.67870, \"train acc: \"0.53966\n",
            "1: \"train loss:\"0.67302, \"train acc: \"0.54327\n",
            "1: \"train loss:\"0.67323, \"train acc: \"0.53486\n",
            "1: \"train loss:\"0.68704, \"train acc: \"0.54207\n",
            "1: \"train loss:\"0.68878, \"train acc: \"0.55409\n",
            "1: \"train loss:\"0.68862, \"train acc: \"0.53365\n",
            "1: \"train loss:\"0.67343, \"train acc: \"0.56490\n",
            "1: \"train loss:\"0.67785, \"train acc: \"0.54447\n",
            "1: \"train loss:\"0.67662, \"train acc: \"0.54207\n",
            "1: \"train loss:\"0.68077, \"train acc: \"0.54928\n",
            "1: \"train loss:\"0.67327, \"train acc: \"0.57812\n",
            "1: \"train loss:\"0.67732, \"train acc: \"0.53846\n",
            "1: \"train loss:\"0.68088, \"train acc: \"0.55529\n",
            "1: \"train loss:\"0.68313, \"train acc: \"0.53606\n",
            "1: \"train loss:\"0.68146, \"train acc: \"0.53726\n",
            "1: \"train loss:\"0.67601, \"train acc: \"0.53486\n",
            "1: \"train loss:\"0.67662, \"train acc: \"0.56370\n",
            "1: \"train loss:\"0.67405, \"train acc: \"0.55048\n",
            "1: \"train loss:\"0.67146, \"train acc: \"0.56370\n",
            "1: \"train loss:\"0.67039, \"train acc: \"0.55649\n",
            "1: \"train loss:\"0.68644, \"train acc: \"0.53365\n",
            "1: \"train loss:\"0.68207, \"train acc: \"0.57452\n",
            "1: \"train loss:\"0.67764, \"train acc: \"0.54688\n",
            "1: \"train loss:\"0.68307, \"train acc: \"0.54567\n",
            "1: \"train loss:\"0.67557, \"train acc: \"0.54087\n",
            "1: \"train loss:\"0.68000, \"train acc: \"0.55048\n",
            "1: \"train loss:\"0.67700, \"train acc: \"0.55288\n",
            "1: \"train loss:\"0.67759, \"train acc: \"0.53606\n",
            "1: \"train loss:\"0.67460, \"train acc: \"0.55889\n",
            "1: \"train loss:\"0.68554, \"train acc: \"0.53846\n",
            "1: \"train loss:\"0.66574, \"train acc: \"0.56611\n",
            "1: \"train loss:\"0.68277, \"train acc: \"0.54327\n",
            "1: \"train loss:\"0.67444, \"train acc: \"0.54207\n",
            "1: \"train loss:\"0.67195, \"train acc: \"0.55889\n",
            "1: \"train loss:\"0.66915, \"train acc: \"0.55409\n",
            "1: \"train loss:\"0.68711, \"train acc: \"0.55529\n",
            "1: \"train loss:\"0.67620, \"train acc: \"0.55529\n",
            "1: \"train loss:\"0.67057, \"train acc: \"0.55168\n",
            "1: \"train loss:\"0.67154, \"train acc: \"0.55769\n",
            "1: \"train loss:\"0.66719, \"train acc: \"0.57091\n",
            "1: \"train loss:\"0.68057, \"train acc: \"0.53966\n",
            "1: \"train loss:\"0.67434, \"train acc: \"0.55769\n",
            "1: \"train loss:\"0.67491, \"train acc: \"0.56851\n",
            "1: \"train loss:\"0.66357, \"train acc: \"0.55889\n",
            "1: \"train loss:\"0.67277, \"train acc: \"0.55409\n",
            "1: \"train loss:\"0.67453, \"train acc: \"0.55288\n",
            "1: \"train loss:\"0.67358, \"train acc: \"0.55769\n",
            "1: \"train loss:\"0.67417, \"train acc: \"0.55288\n",
            "1: \"train loss:\"0.67972, \"train acc: \"0.55288\n",
            "1: \"train loss:\"0.66951, \"train acc: \"0.55649\n",
            "1: \"train loss:\"0.68674, \"train acc: \"0.55529\n",
            "1: \"train loss:\"0.66396, \"train acc: \"0.55889\n",
            "1: \"train loss:\"0.67129, \"train acc: \"0.55529\n",
            "1: \"train loss:\"0.65746, \"train acc: \"0.56490\n",
            "1: \"train loss:\"0.66837, \"train acc: \"0.57212\n",
            "1: \"train loss:\"0.67581, \"train acc: \"0.54207\n",
            "1: \"train loss:\"0.66622, \"train acc: \"0.55649\n",
            "1: \"train loss:\"0.67180, \"train acc: \"0.55288\n",
            "1: \"train loss:\"0.66942, \"train acc: \"0.54928\n",
            "1: \"train loss:\"0.66174, \"train acc: \"0.56490\n",
            "1: \"train loss:\"0.67652, \"train acc: \"0.56370\n",
            "1: \"train loss:\"0.67436, \"train acc: \"0.54808\n",
            "1: \"train loss:\"0.67912, \"train acc: \"0.55168\n",
            "1: \"train loss:\"0.66146, \"train acc: \"0.57212\n",
            "1: \"train loss:\"0.66964, \"train acc: \"0.55889\n",
            "1: \"train loss:\"0.67820, \"train acc: \"0.52644\n",
            "1: \"train loss:\"0.67710, \"train acc: \"0.55889\n",
            "1: \"train loss:\"0.67904, \"train acc: \"0.55409\n",
            "1: \"train loss:\"0.67190, \"train acc: \"0.54567\n",
            "1: \"train loss:\"0.66815, \"train acc: \"0.57332\n",
            "1: \"train loss:\"0.67972, \"train acc: \"0.55168\n",
            "1: \"train loss:\"0.68203, \"train acc: \"0.55048\n",
            "1: \"train loss:\"0.67595, \"train acc: \"0.54567\n",
            "1: \"train loss:\"0.67107, \"train acc: \"0.55168\n",
            "1: \"train loss:\"0.66043, \"train acc: \"0.56731\n",
            "1: \"train loss:\"0.66938, \"train acc: \"0.56010\n",
            "1: \"train loss:\"0.66294, \"train acc: \"0.57452\n",
            "1: \"train loss:\"0.67458, \"train acc: \"0.54087\n",
            "1: \"val loss:\"0.66805, \"val acc: \"0.55048\n",
            "1: \"val loss:\"0.66407, \"val acc: \"0.59135\n",
            "1: \"val loss:\"0.66389, \"val acc: \"0.55048\n",
            "1: \"val loss:\"0.67931, \"val acc: \"0.54567\n",
            "1: \"val loss:\"0.67458, \"val acc: \"0.55048\n",
            "1: \"val loss:\"0.67477, \"val acc: \"0.56490\n",
            "1: \"val loss:\"0.66163, \"val acc: \"0.57692\n",
            "1: \"val loss:\"0.65933, \"val acc: \"0.57692\n",
            "1: \"val loss:\"0.67124, \"val acc: \"0.56490\n",
            "1: \"val loss:\"0.66347, \"val acc: \"0.57692\n",
            "1: \"val loss:\"0.68639, \"val acc: \"0.52163\n",
            "1: \"val loss:\"0.67428, \"val acc: \"0.55048\n",
            "1: \"val loss:\"0.66486, \"val acc: \"0.56250\n",
            "1: \"val loss:\"0.67785, \"val acc: \"0.56971\n",
            "1: \"val loss:\"0.69023, \"val acc: \"0.55048\n",
            "1: \"val loss:\"0.68287, \"val acc: \"0.56010\n",
            "1: \"val loss:\"0.68647, \"val acc: \"0.53365\n",
            "1: \"val loss:\"0.66697, \"val acc: \"0.58894\n",
            "1: \"val loss:\"0.65891, \"val acc: \"0.59135\n",
            "1: \"val loss:\"0.66962, \"val acc: \"0.56250\n",
            "1: \"val loss:\"0.64958, \"val acc: \"0.59856\n",
            "1: \"val loss:\"0.66947, \"val acc: \"0.57452\n",
            "1: \"val loss:\"0.67012, \"val acc: \"0.58173\n",
            "1: \"val loss:\"0.68618, \"val acc: \"0.54567\n",
            "1: \"val loss:\"0.66504, \"val acc: \"0.56010\n",
            "1: \"val loss:\"0.66987, \"val acc: \"0.58413\n",
            "1: \"val loss:\"0.67266, \"val acc: \"0.56731\n",
            "1: \"val loss:\"0.66863, \"val acc: \"0.57452\n",
            "1: \"val loss:\"0.66882, \"val acc: \"0.57692\n",
            "1: \"val loss:\"0.68664, \"val acc: \"0.54567\n",
            "1: \"val loss:\"0.66284, \"val acc: \"0.56971\n",
            "1: \"val loss:\"0.67379, \"val acc: \"0.56250\n",
            "1: \"val loss:\"0.69389, \"val acc: \"0.52404\n",
            "1: \"val loss:\"0.66566, \"val acc: \"0.57452\n",
            "1: \"val loss:\"0.67633, \"val acc: \"0.54327\n",
            "1: \"val loss:\"0.66163, \"val acc: \"0.56010\n",
            "1: \"val loss:\"0.66250, \"val acc: \"0.56250\n",
            "1: \"val loss:\"0.65707, \"val acc: \"0.55769\n",
            "1: \"val loss:\"0.68024, \"val acc: \"0.57452\n",
            "1: \"val loss:\"0.65975, \"val acc: \"0.58173\n",
            "1: \"val loss:\"0.67660, \"val acc: \"0.55769\n",
            "1: \"val loss:\"0.68278, \"val acc: \"0.56010\n",
            "1: \"val loss:\"0.66701, \"val acc: \"0.54567\n",
            "1: \"val loss:\"0.66473, \"val acc: \"0.55529\n",
            "1: \"val loss:\"0.65604, \"val acc: \"0.56490\n",
            "1: \"val loss:\"0.67052, \"val acc: \"0.55048\n",
            "1: \"val loss:\"0.65859, \"val acc: \"0.56731\n",
            "1: \"val loss:\"0.66946, \"val acc: \"0.56731\n",
            "1: \"val loss:\"0.68580, \"val acc: \"0.56250\n",
            "1: \"val loss:\"0.65603, \"val acc: \"0.56250\n",
            "1: \"val loss:\"0.68502, \"val acc: \"0.55529\n",
            "1: \"val loss:\"0.66689, \"val acc: \"0.56971\n",
            "1: \"val loss:\"0.64928, \"val acc: \"0.60096\n",
            "1: \"val loss:\"0.66975, \"val acc: \"0.58413\n",
            "1: \"val loss:\"0.67764, \"val acc: \"0.55769\n",
            "1: \"val loss:\"0.66906, \"val acc: \"0.56490\n",
            "1: \"val loss:\"0.66324, \"val acc: \"0.57692\n",
            "1: \"val loss:\"0.67941, \"val acc: \"0.54567\n",
            "1: \"val loss:\"0.66238, \"val acc: \"0.57212\n",
            "1: \"val loss:\"0.66592, \"val acc: \"0.57692\n",
            "1: \"val loss:\"0.69592, \"val acc: \"0.52404\n",
            "1: \"val loss:\"0.66429, \"val acc: \"0.54808\n",
            "2: \"train loss:\"0.67521, \"train acc: \"0.53486\n",
            "2: \"train loss:\"0.66492, \"train acc: \"0.54567\n",
            "2: \"train loss:\"0.65954, \"train acc: \"0.57212\n",
            "2: \"train loss:\"0.66120, \"train acc: \"0.56731\n",
            "2: \"train loss:\"0.66123, \"train acc: \"0.59375\n"
          ]
        }
      ],
      "source": [
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.MultiLabelSoftMarginLoss()\n",
        "    #scheduler 추가 및 step 추가\n",
        "\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)   \n",
        "num_epochs = 10\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_acc_list = []\n",
        "    valid_acc_list =[]\n",
        "    for i, (images, targets) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        images = images.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "            outputs = outputs > 0.5\n",
        "            acc = (outputs == targets).float().mean()\n",
        "            train_acc_list.append(acc)\n",
        "\n",
        "            print(f'{epoch}: \"train loss:\"{loss.item():.5f}, \"train acc: \"{acc.item():.5f}')\n",
        "    for i, (images, targets) in enumerate(valid_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        images = images.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        valid_loss = criterion(outputs, targets)\n",
        "      \n",
        "        if (i+1) % 10 == 0:\n",
        "            outputs = outputs > 0.5\n",
        "            valid_acc = (outputs == targets).float().mean()\n",
        "            valid_acc_list.append(acc)\n",
        "\n",
        "            print(f'{epoch}: \"val loss:\"{valid_loss.item():.5f}, \"val acc: \"{valid_acc.item():.5f}')\n",
        "    lr_scheduler.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeYvHfUtBtpz"
      },
      "source": [
        "## 5. 추론하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSfp-slSBtp0"
      },
      "outputs": [],
      "source": [
        "submit = pd.read_csv('data/sample_submission.csv')\n",
        "\n",
        "model.eval()\n",
        "batch_size = test_loader.batch_size\n",
        "batch_index = 0\n",
        "for i, (images, targets) in enumerate(test_loader):\n",
        "    images = images.to(device)\n",
        "    targets = targets.to(device)\n",
        "    outputs = model(images)\n",
        "    outputs = outputs > 0.5\n",
        "    batch_index = i * batch_size\n",
        "    submit.iloc[batch_index:batch_index+batch_size, 1:] = \\\n",
        "        outputs.long().squeeze(0).detach().cpu().numpy()\n",
        "    \n",
        "submit.to_csv('submit.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL0SuxUHUirr"
      },
      "outputs": [],
      "source": [
        "import matplotlib as plt\n",
        "\n",
        "plt.plot(train_acc_list)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "파이토치를_이용한 정말 간단한 Multi-Label Classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}